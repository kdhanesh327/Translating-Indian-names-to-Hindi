{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-24 00:22:23--  https://docs.google.com/spreadsheets/d/1JpK9nOuZ2ctMrjNL-C0ghUQ4TesTrMER1-dTD_torAA/gviz/tq?tqx=out:csv&sheet=data.train.csv\n",
      "Resolving docs.google.com (docs.google.com)... 142.250.192.14, 2404:6800:4009:832::200e\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.192.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘data.train.csv’\n",
      "\n",
      "data.train.csv          [ <=>                ] 126.35K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2024-09-24 00:22:24 (1.42 MB/s) - ‘data.train.csv’ saved [129381]\n",
      "\n",
      "--2024-09-24 00:22:26--  https://docs.google.com/spreadsheets/d/1cKC0WpWpIQJkaqnFb7Ou7d0syFDsj6eEW7bM7GH3u2k/gviz/tq?tqx=out:csv&sheet=data.valid.csv\n",
      "Resolving docs.google.com (docs.google.com)... 142.250.192.14, 2404:6800:4009:832::200e\n",
      "Connecting to docs.google.com (docs.google.com)|142.250.192.14|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘data.valid.csv’\n",
      "\n",
      "data.valid.csv          [ <=>                ]  11.29K  --.-KB/s    in 0.006s  \n",
      "\n",
      "2024-09-24 00:22:27 (1.91 MB/s) - ‘data.valid.csv’ saved [11560]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import subprocess\n",
    "import collections\n",
    "import unicodedata\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 3rd-party package imports, may require installation if not on a platform such as Colab.\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "import pandas as pd\n",
    "import tqdm.auto as tqdm\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "from nltk.translate import bleu_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Please do not change anything in the following cell\n",
    "\n",
    "# Find and load fonts that can display Hindi characters, for Matplotlib\n",
    "result = subprocess.run([ 'fc-list', ':lang=hi', 'family' ], capture_output=True)\n",
    "found_hindi_fonts = result.stdout.decode('utf-8').strip().split('\\n')\n",
    "\n",
    "matplotlib.rcParams['font.sans-serif'] = [\n",
    "    'Source Han Sans TW', 'sans-serif', 'Arial Unicode MS',\n",
    "    *found_hindi_fonts\n",
    "]\n",
    "\n",
    "# Please do not change anything in the following cell\n",
    "STUDENT_SAP_NAME = 'Dhanesh'\n",
    "STUDENT_SR_NUMBER = 22476\n",
    "\n",
    "DIRECTORY_NAME = f\"{STUDENT_SAP_NAME.replace(' ', '_')}_{STUDENT_SR_NUMBER}\"\n",
    "\n",
    "# os.makedirs(DIRECTORY_NAME, exist_ok=True)\n",
    "\n",
    "def sync_vram():\n",
    "    \"\"\" Synchronizes the VRAM across the GPUs, reclaiming unused memory. \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "\"\"\"## Data Preparation\n",
    "\n",
    "We'll load the data for the task, which comprises of a parallel corpus of Indian Names and their Hindi equivalents.\n",
    "\"\"\"\n",
    "\n",
    "# Make sure your code is not dependent on any of the file names as below.\n",
    "\n",
    "# Download the training and validation datasets\n",
    "!wget -O data.train.csv \"https://docs.google.com/spreadsheets/d/1JpK9nOuZ2ctMrjNL-C0ghUQ4TesTrMER1-dTD_torAA/gviz/tq?tqx=out:csv&sheet=data.train.csv\"\n",
    "!wget -O data.valid.csv \"https://docs.google.com/spreadsheets/d/1cKC0WpWpIQJkaqnFb7Ou7d0syFDsj6eEW7bM7GH3u2k/gviz/tq?tqx=out:csv&sheet=data.valid.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data: 4484\n",
      "Length of validation data: 400\n"
     ]
    }
   ],
   "source": [
    "# Please do not change anything in the following cell\n",
    "\n",
    "def read_dataframe(ds_type):\n",
    "    \"\"\" Loads a dataframe based on the given partition type.\n",
    "\n",
    "    Args:\n",
    "        ds_type (str): Dataset type: train (train) or validation (valid)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Pandas Dataframe for the specified partition.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(f\"data.{ds_type}.csv\", header=0)\n",
    "    df = df[~df.isna()]\n",
    "    df['Name'] = df['Name'].astype(str)\n",
    "    df['Translation'] = df['Translation'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Load the training and validation datasets\n",
    "train_data      = read_dataframe(\"train\")\n",
    "validation_data = read_dataframe(\"valid\")\n",
    "\n",
    "print(f\"Length of training data: {len(train_data)}\\nLength of validation data: {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Here are some examples from the training dataset. Note that the dataset may be noisy so some examples may not be perfect:\"\"\"\n",
    "\n",
    "train_data.sample(n=5)\n",
    "\n",
    "\"\"\"## Tokenization\n",
    "\n",
    "Now with the data, you must first prepare a tokenization strategy for feeding name pairs as a sequence to different models. For English this could be as simple as using individual characters as tokens, but Hindi has accents (मात्राएँ), a larger set of vowels (स्वर), consonants (व्यंजन), and additional composition rules (half-letters, etc.), so such a simple strategy may not be effective.\n",
    "\n",
    "In NLP literature, multiple strategies exist for automatically learning a suitable sub-word tokenization strategy from the given data. Such tokenizers exist in two types:\n",
    "- Given a set of initial tokens, learn suitable combinations which are added as new tokens until a certain vocabulary size is reached. Examples of these include [BPE Tokenization](https://arxiv.org/abs/1508.07909) and [WordPiece Tokenization, introduced by the BERT paper](https://arxiv.org/abs/1810.04805).\n",
    "- Given a large set of initial tokens, learn suitable rules to reduce the size of the vocabulary to a desired size. An example of this includes [SentencePiece Tokenization](https://arxiv.org/abs/1808.06226).\n",
    "\n",
    "Given empirical results, these are popular strategies to learn tokenization automatically from given data.\n",
    "\n",
    "In this section, you will implement a tokenizer for the given data. There could be multiple strategies to implement tokenizers:\n",
    "- You can have a tokenizer that operates jointly over both languages or have separate tokenizers for English and Hindi.\n",
    "- Your tokenizer can learn the tokenization from data (using any one of the techniques mentioned above) or can use a fixed set of rules for decomposition.\n",
    "\n",
    "Implement the logic for any tokenizer of your choice in the class `Tokenizer` below. Make sure to adhere to the rules and constraints in the docstrings.\n",
    "\n",
    "The tokenizer will learn a mapping of tokens to ids and vice versa and use these to map strings. This mapping can be built based on merge rules (BPE, WordPiece, etc.) or hand-crafted rules, in the `Tokenizer.train()` function. Additionally the tokenizer will also handle preprocessing and postprocessing of strings during the encoding phase (string to tokens).\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\" Represents the tokenizer for text data.\n",
    "        Provides methods to encode and decode strings (as instance or as a batch). \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes a new tokenizer.\n",
    "\n",
    "            Any variables required in intermediate operations are declared here.\n",
    "            You will also need to define things like special tokens and other things here.\n",
    "\n",
    "            All variables declared in this function will be serialized\n",
    "                and deserialized when loading and saving the Tokenizer.\n",
    "            \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.init\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        self.special_tokens = {'<START>' : 0 , '[EOS]' : 1  , '<PAD>' : 2  , '<UNK>' : 3}\n",
    "        self.token_to_id =  self.special_tokens\n",
    "        self.id_to_token = {v:k for k,v in self.token_to_id.items()}\n",
    "        self.merges = {}\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        \"\"\" Loads a pre-trained tokenizer from the given directory.\n",
    "           This directory will have a tokenizer.pkl file that contains all the tokenizer variables.\n",
    "\n",
    "        Args:\n",
    "            path (str): Path to load the tokenizer from.\n",
    "        \"\"\"\n",
    "        tokenizer_file = os.path.join(path, \"tokenizer.pkl\")\n",
    "\n",
    "        if not os.path.exists(path) or not os.path.exists(os.path.join(path, \"tokenizer.pkl\")):\n",
    "            raise ValueError(cls.load.__name__ + \": No tokenizer found at the specified directory\")\n",
    "\n",
    "        with open(tokenizer_file, \"rb\") as ifile:\n",
    "            return pickle.load(ifile)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\" Saves a trained tokenizer to a given directory, inside a tokenizer.pkl file.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory to save the tokenizer in.\n",
    "        \"\"\"\n",
    "\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path, \"tokenizer.pkl\"), 'wb') as ofile:\n",
    "            pickle.dump(self, ofile)\n",
    "\n",
    "    def split_text_with_punctuation(self,text):\n",
    "        # Define common punctuation marks for both English and Hindi\n",
    "        punctuation_marks = r'[.,\\/#!$%\\^&\\*;:{}=\\-_`~()\\[\\]\\“\\”\\\"\\'\\’]'\n",
    "\n",
    "        # Split text at punctuation marks\n",
    "        segments = re.split(punctuation_marks, text)\n",
    "\n",
    "        # Initialize list for characters\n",
    "        characters = []\n",
    "\n",
    "        # Iterate over segments\n",
    "        for segment in segments:\n",
    "            # Split segment into characters\n",
    "            chars = [char for char in segment if char != ' ']\n",
    "            # Append characters to list\n",
    "            characters.extend(chars)\n",
    "\n",
    "        return characters\n",
    "\n",
    "    def get_stats(self,ids):\n",
    "        counts = {}\n",
    "        for lst in ids:\n",
    "            for pair in zip(lst,lst[1:]):\n",
    "                counts[pair] = counts.get(pair,0)+1\n",
    "\n",
    "        # for pair in zip(ids, ids[1:]): # Pythonic way to iterate consecutive elements\n",
    "        #     counts[pair] = counts.get(pair, 0) + 1\n",
    "        return counts\n",
    "\n",
    "    def merge(self,ids, pair, idx):\n",
    "        newids = []\n",
    "        i = 0\n",
    "        # print(pair)\n",
    "        for lst in ids:\n",
    "            # print(lst)\n",
    "            temp = []\n",
    "            i = 0\n",
    "            while i < len(lst):\n",
    "                if i < len(lst) - 1 and lst[i] == pair[0] and lst[i+1] == pair[1]:\n",
    "                    temp.append(idx)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    temp.append(lst[i])\n",
    "                    i += 1\n",
    "            # print(temp)\n",
    "            newids.append(temp)\n",
    "        return newids\n",
    "\n",
    "    def initialencode(self,name):\n",
    "        tokens = []\n",
    "        # tokens.append(self.token_to_id['<START>'])\n",
    "        characters = self.split_text_with_punctuation(name)\n",
    "        for char in characters:\n",
    "            if char not in self.token_to_id:\n",
    "                tokens.append(self.token_to_id['<UNK>'])\n",
    "            else:\n",
    "                tokens.append(self.token_to_id[char])\n",
    "        # tokens.append(self.token_to_id['<END>'])\n",
    "        return tokens\n",
    "\n",
    "    def train(self, data, vocab_size):\n",
    "        \"\"\" Trains a tokenizer to learn meaningful representations from input data.\n",
    "            In the end, learns a vocabulary of a fixed size over the given data.\n",
    "            Special tokens, if any, must not be counted towards this vocabulary.\n",
    "\n",
    "        Args:\n",
    "            data (list[str]): List of input strings from a text corpus.\n",
    "            vocab_size (int): Final desired size of the vocab to be learnt.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.train\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        n = 4\n",
    "\n",
    "        # print(data)\n",
    "\n",
    "        for name in data:\n",
    "            characters = self.split_text_with_punctuation(name)\n",
    "            for char in characters:\n",
    "                if char not in self.token_to_id:\n",
    "                    self.token_to_id[char] = n\n",
    "                    self.id_to_token[n] = char\n",
    "                    n = n+1\n",
    "\n",
    "        token_data = []\n",
    "\n",
    "        for name in data:\n",
    "            tokens = self.initialencode(name)\n",
    "            token_data.append(tokens)\n",
    "\n",
    "        ids = token_data # copy so we don't destroy the original list\n",
    "\n",
    "        num_merges = abs(len(self.token_to_id) - vocab_size)\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            stats = self.get_stats(ids)\n",
    "            pair = max(stats, key=stats.get)\n",
    "            idx = n + i\n",
    "\n",
    "            ch = self.id_to_token[pair[0]] + self.id_to_token[pair[1]]\n",
    "\n",
    "            self.token_to_id[ch] = idx\n",
    "            self.id_to_token[idx] = ch\n",
    "\n",
    "            print(f\"merging {pair} into a new token {idx}\")\n",
    "            ids = self.merge(ids, pair, idx)\n",
    "\n",
    "            self.merges[pair] = idx\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "\n",
    "    def pad(self, tokens, length):\n",
    "        \"\"\" Pads a tokenized string to a specified length, for batch processing.\n",
    "\n",
    "        Args:\n",
    "            tokens (list[int]): Encoded token string to be padded.\n",
    "            length (int): Length of tokens to pad to.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: Token string padded to desired length.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.pad\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        paded_tokens = tokens[:length] + [self.special_tokens[\"<PAD>\"]] * max(0,length-len(tokens))\n",
    "        return paded_tokens\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def unpad(self, tokens):\n",
    "        \"\"\" Removes padding from a token string.\n",
    "\n",
    "        Args:\n",
    "            tokens (list[int]): Encoded token string with padding.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: Token string with padding removed.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.unpad\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        return [token for token in tokens if token != self.special_tokens[\"<PAD>\"]]\n",
    "\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def get_special_tokens(self):\n",
    "        \"\"\" Returns the associated special tokens.\n",
    "\n",
    "            Returns:\n",
    "                dict[str, int]: Mapping describing the special tokens, if any.\n",
    "                    This is a mapping between a string segment (token) and its associated id (token_id).\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.get_special_tokens\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        return self.special_tokens\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        \"\"\" Returns the learnt vocabulary post the training process.\n",
    "\n",
    "            Returns:\n",
    "                dict[str, int]: Mapping describing the vocabulary and special tokens, if any.\n",
    "                    This is a mapping between a string segment (token) and its associated id (token_id).\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.get_vocabulary\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        return self.token_to_id\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def encode(self, string, add_start=True, add_end=True):\n",
    "        \"\"\" Encodes a string into a list of tokens.\n",
    "\n",
    "        Args:\n",
    "            string (str): Input string to be tokenized.\n",
    "            add_start (bool): If true, adds the start of sequence token.\n",
    "            add_end (bool): If true, adds the end of sequence token.\n",
    "        Returns:\n",
    "            list[int]: List of tokens (unpadded).\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.encode\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        tokens = []\n",
    "        if(add_start):\n",
    "            tokens.append(self.token_to_id['<START>'])\n",
    "\n",
    "        string = self.split_text_with_punctuation(string)\n",
    "\n",
    "        subword = \"\"\n",
    "\n",
    "        for char in string:\n",
    "            subword = subword + char\n",
    "            # print(subword)\n",
    "            if subword not in self.token_to_id:\n",
    "                if subword[:-1] not in self.token_to_id:\n",
    "                    tokens.append(self.token_to_id[\"<UNK>\"])\n",
    "                else:\n",
    "                    tokens.append(self.token_to_id[subword[:-1]])\n",
    "                subword = \"\"\n",
    "                subword += char\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "\n",
    "        if(subword in self.token_to_id):\n",
    "            tokens.append(self.token_to_id[subword])\n",
    "        else:\n",
    "            tokens.append(self.token_to_id[\"<UNK>\"])\n",
    "        # else:\n",
    "        #     if subword[:-1] != \"\":\n",
    "        #         tokens.append(self.token_to_id[subword[:-1]])\n",
    "        #         if subword[-1] in self.token_to_id:\n",
    "        #             tokens.append(self.token_to_id[char])\n",
    "        #         else:\n",
    "        #             tokens.append(self.token_to_id[\"<UNK>\"])\n",
    "        #     else:\n",
    "        #         if subword[-1] in self.token_to_id:\n",
    "        #             tokens.append(self.token_to_id[char])\n",
    "\n",
    "        if(add_end):\n",
    "            tokens.append(self.token_to_id['[EOS]'])\n",
    "\n",
    "        return tokens\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def decode(self, tokens, strip_special=True):\n",
    "        \"\"\" Decodes a string from a list of tokens.\n",
    "            Undoes the tokenization, returning back the input string.\n",
    "\n",
    "        Args:\n",
    "            tokens (list[int]): List of encoded tokens to be decoded. No padding is assumed.\n",
    "            strip_special (bool): Whether to remove special tokens or not.\n",
    "\n",
    "        Returns:\n",
    "            str: Decoded string.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : tokenizer.decode\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        if(strip_special == True):\n",
    "            tokens = tokens[1:-1]\n",
    "\n",
    "        text = \"\"\n",
    "\n",
    "        for idx in tokens:\n",
    "            if idx in self.id_to_token and self.id_to_token[idx] != '<PAD>':\n",
    "                text = text+self.id_to_token[idx]\n",
    "\n",
    "        # text = \"\".join(self.id_to_token[idx] for idx in tokens)\n",
    "        # text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "        return text\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "\n",
    "    def batch_encode(self, batch, padding=None, add_start=True, add_end=True):\n",
    "        \"\"\"Encodes multiple strings in a batch to list of tokens padded to a given size.\n",
    "\n",
    "        Args:\n",
    "            batch (list[str]): List of strings to be tokenized.\n",
    "            padding (int, optional): Optional, desired tokenized length. Outputs will be padded to fit this length.\n",
    "            add_start (bool): If true, adds the start of sequence token.\n",
    "            add_end (bool): If true, adds the end of sequence token.\n",
    "\n",
    "        Returns:\n",
    "            list[list[int]]: List of tokenized outputs, padded to the same length.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_output = [ self.encode(string, add_start, add_end) for string in batch ]\n",
    "        if padding:\n",
    "            for i, tokens in enumerate(batch_output):\n",
    "                if len(tokens) < padding:\n",
    "                    batch_output[i] = self.pad(tokens, padding)\n",
    "        return batch_output\n",
    "\n",
    "    def batch_decode(self, batch, strip_special=True):\n",
    "        \"\"\" Decodes a batch of encoded tokens to normal strings.\n",
    "\n",
    "        Args:\n",
    "            batch (list[list[int]]): List of encoded token strings, optionally padded.\n",
    "            strip_special (bool): Whether to remove special tokens or not.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: Decoded strings after padding is removed.\n",
    "        \"\"\"\n",
    "        return [ self.decode(self.unpad(tokens), strip_special=strip_special) for tokens in batch ]\n",
    "\n",
    "## ==== END EVALUATION PORTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (4, 14) into a new token 32\n",
      "merging (10, 6) into a new token 33\n",
      "merging (11, 4) into a new token 34\n",
      "merging (4, 11) into a new token 35\n",
      "merging (4, 12) into a new token 36\n",
      "merging (9, 9) into a new token 37\n",
      "merging (7, 14) into a new token 38\n",
      "merging (4, 21) into a new token 39\n",
      "merging (14, 4) into a new token 40\n",
      "merging (6, 4) into a new token 41\n",
      "merging (10, 4) into a new token 42\n",
      "merging (21, 4) into a new token 43\n",
      "merging (7, 16) into a new token 44\n",
      "merging (10, 24) into a new token 45\n",
      "merging (13, 4) into a new token 46\n",
      "merging (33, 4) into a new token 47\n",
      "merging (8, 9) into a new token 48\n",
      "merging (23, 6) into a new token 49\n",
      "merging (18, 4) into a new token 50\n",
      "merging (12, 4) into a new token 51\n",
      "merging (34, 21) into a new token 52\n",
      "merging (20, 4) into a new token 53\n",
      "merging (16, 7) into a new token 54\n",
      "merging (19, 7) into a new token 55\n",
      "merging (9, 14) into a new token 56\n",
      "merging (11, 7) into a new token 57\n",
      "merging (8, 6) into a new token 58\n",
      "merging (16, 4) into a new token 59\n",
      "merging (33, 7) into a new token 60\n",
      "merging (5, 6) into a new token 61\n",
      "merging (21, 32) into a new token 62\n",
      "merging (5, 4) into a new token 63\n",
      "merging (9, 33) into a new token 64\n",
      "merging (7, 11) into a new token 65\n",
      "merging (8, 4) into a new token 66\n",
      "merging (24, 12) into a new token 67\n",
      "merging (48, 11) into a new token 68\n",
      "merging (34, 18) into a new token 69\n",
      "merging (24, 14) into a new token 70\n",
      "merging (20, 6) into a new token 71\n",
      "merging (10, 32) into a new token 72\n",
      "merging (8, 7) into a new token 73\n",
      "merging (22, 22) into a new token 74\n",
      "merging (10, 7) into a new token 75\n",
      "merging (25, 4) into a new token 76\n",
      "merging (24, 11) into a new token 77\n",
      "merging (32, 8) into a new token 78\n",
      "merging (20, 7) into a new token 79\n",
      "merging (14, 7) into a new token 80\n",
      "merging (32, 4) into a new token 81\n",
      "merging (38, 4) into a new token 82\n",
      "merging (37, 16) into a new token 83\n",
      "merging (6, 35) into a new token 84\n",
      "merging (6, 7) into a new token 85\n",
      "merging (25, 36) into a new token 86\n",
      "merging (44, 4) into a new token 87\n",
      "merging (5, 41) into a new token 88\n",
      "merging (32, 7) into a new token 89\n",
      "merging (21, 22) into a new token 90\n",
      "merging (24, 8) into a new token 91\n",
      "merging (19, 4) into a new token 92\n",
      "merging (5, 7) into a new token 93\n",
      "merging (25, 11) into a new token 94\n",
      "merging (36, 7) into a new token 95\n",
      "merging (56, 8) into a new token 96\n",
      "merging (25, 35) into a new token 97\n",
      "merging (11, 24) into a new token 98\n",
      "merging (10, 22) into a new token 99\n",
      "merging (10, 35) into a new token 100\n",
      "merging (20, 35) into a new token 101\n",
      "merging (21, 7) into a new token 102\n",
      "merging (19, 37) into a new token 103\n",
      "merging (24, 10) into a new token 104\n",
      "merging (33, 32) into a new token 105\n",
      "merging (25, 34) into a new token 106\n",
      "merging (17, 4) into a new token 107\n",
      "merging (21, 9) into a new token 108\n",
      "merging (8, 37) into a new token 109\n",
      "merging (16, 6) into a new token 110\n",
      "merging (20, 24) into a new token 111\n",
      "merging (18, 7) into a new token 112\n",
      "merging (56, 68) into a new token 113\n",
      "merging (4, 4) into a new token 114\n",
      "merging (12, 36) into a new token 115\n",
      "merging (12, 7) into a new token 116\n",
      "merging (38, 68) into a new token 117\n",
      "merging (11, 32) into a new token 118\n",
      "merging (11, 22) into a new token 119\n",
      "merging (8, 38) into a new token 120\n",
      "merging (37, 14) into a new token 121\n",
      "merging (35, 7) into a new token 122\n",
      "merging (49, 78) into a new token 123\n",
      "merging (20, 32) into a new token 124\n",
      "merging (96, 34) into a new token 125\n",
      "merging (5, 65) into a new token 126\n",
      "merging (6, 32) into a new token 127\n",
      "merging (18, 83) into a new token 128\n",
      "merging (9, 11) into a new token 129\n",
      "merging (18, 32) into a new token 130\n",
      "merging (21, 24) into a new token 131\n",
      "merging (13, 32) into a new token 132\n",
      "merging (17, 22) into a new token 133\n",
      "merging (33, 24) into a new token 134\n",
      "merging (36, 4) into a new token 135\n",
      "merging (48, 19) into a new token 136\n",
      "merging (10, 39) into a new token 137\n",
      "merging (7, 33) into a new token 138\n",
      "merging (109, 25) into a new token 139\n",
      "merging (38, 7) into a new token 140\n",
      "merging (5, 24) into a new token 141\n",
      "merging (16, 24) into a new token 142\n",
      "merging (103, 11) into a new token 143\n",
      "merging (16, 32) into a new token 144\n",
      "merging (6, 9) into a new token 145\n",
      "merging (18, 22) into a new token 146\n",
      "merging (7, 12) into a new token 147\n",
      "merging (20, 41) into a new token 148\n",
      "merging (91, 120) into a new token 149\n",
      "merging (25, 32) into a new token 150\n",
      "merging (18, 44) into a new token 151\n",
      "merging (34, 16) into a new token 152\n",
      "merging (18, 24) into a new token 153\n",
      "merging (25, 24) into a new token 154\n",
      "merging (61, 24) into a new token 155\n",
      "merging (20, 39) into a new token 156\n",
      "merging (42, 16) into a new token 157\n",
      "merging (11, 9) into a new token 158\n",
      "merging (19, 38) into a new token 159\n",
      "merging (8, 41) into a new token 160\n",
      "merging (14, 9) into a new token 161\n",
      "merging (19, 9) into a new token 162\n",
      "merging (50, 13) into a new token 163\n",
      "merging (16, 35) into a new token 164\n",
      "merging (17, 32) into a new token 165\n",
      "merging (14, 24) into a new token 166\n",
      "merging (17, 6) into a new token 167\n",
      "merging (33, 22) into a new token 168\n",
      "merging (13, 39) into a new token 169\n",
      "merging (21, 70) into a new token 170\n",
      "merging (43, 6) into a new token 171\n",
      "merging (14, 35) into a new token 172\n",
      "merging (5, 36) into a new token 173\n",
      "merging (61, 7) into a new token 174\n",
      "merging (20, 9) into a new token 175\n",
      "merging (19, 32) into a new token 176\n",
      "merging (16, 22) into a new token 177\n",
      "merging (33, 9) into a new token 178\n",
      "merging (20, 36) into a new token 179\n",
      "merging (11, 38) into a new token 180\n",
      "merging (21, 36) into a new token 181\n",
      "merging (12, 24) into a new token 182\n",
      "merging (17, 7) into a new token 183\n",
      "merging (12, 22) into a new token 184\n",
      "merging (50, 7) into a new token 185\n",
      "merging (17, 67) into a new token 186\n",
      "merging (18, 39) into a new token 187\n",
      "merging (53, 33) into a new token 188\n",
      "merging (20, 64) into a new token 189\n",
      "merging (22, 21) into a new token 190\n",
      "merging (23, 41) into a new token 191\n",
      "merging (16, 9) into a new token 192\n",
      "merging (55, 11) into a new token 193\n",
      "merging (17, 77) into a new token 194\n",
      "merging (45, 21) into a new token 195\n",
      "merging (14, 37) into a new token 196\n",
      "merging (42, 6) into a new token 197\n",
      "merging (4, 8) into a new token 198\n",
      "merging (44, 7) into a new token 199\n",
      "merging (20, 33) into a new token 200\n",
      "merging (15, 4) into a new token 201\n",
      "merging (21, 35) into a new token 202\n",
      "merging (15, 7) into a new token 203\n",
      "merging (49, 32) into a new token 204\n",
      "merging (5, 32) into a new token 205\n",
      "merging (37, 40) into a new token 206\n",
      "merging (50, 17) into a new token 207\n",
      "merging (20, 65) into a new token 208\n",
      "merging (45, 11) into a new token 209\n",
      "merging (20, 70) into a new token 210\n",
      "merging (10, 9) into a new token 211\n",
      "merging (48, 55) into a new token 212\n",
      "merging (21, 39) into a new token 213\n",
      "merging (20, 22) into a new token 214\n",
      "merging (42, 5) into a new token 215\n",
      "merging (10, 21) into a new token 216\n",
      "merging (24, 33) into a new token 217\n",
      "merging (37, 11) into a new token 218\n",
      "merging (57, 33) into a new token 219\n",
      "merging (8, 77) into a new token 220\n",
      "merging (57, 46) into a new token 221\n",
      "merging (58, 35) into a new token 222\n",
      "merging (24, 16) into a new token 223\n",
      "merging (33, 39) into a new token 224\n",
      "merging (72, 4) into a new token 225\n",
      "merging (33, 35) into a new token 226\n",
      "merging (6, 24) into a new token 227\n",
      "merging (14, 74) into a new token 228\n",
      "merging (39, 35) into a new token 229\n",
      "merging (10, 36) into a new token 230\n",
      "merging (32, 24) into a new token 231\n",
      "merging (5, 38) into a new token 232\n",
      "merging (62, 7) into a new token 233\n",
      "merging (21, 104) into a new token 234\n",
      "merging (58, 7) into a new token 235\n",
      "merging (37, 21) into a new token 236\n",
      "merging (12, 64) into a new token 237\n",
      "merging (84, 7) into a new token 238\n",
      "merging (40, 110) into a new token 239\n",
      "merging (70, 7) into a new token 240\n",
      "merging (5, 9) into a new token 241\n",
      "merging (8, 32) into a new token 242\n",
      "merging (63, 7) into a new token 243\n",
      "merging (17, 24) into a new token 244\n",
      "merging (58, 32) into a new token 245\n",
      "merging (12, 9) into a new token 246\n",
      "merging (14, 39) into a new token 247\n",
      "merging (13, 22) into a new token 248\n",
      "merging (90, 6) into a new token 249\n",
      "merging (34, 19) into a new token 250\n",
      "merging (14, 32) into a new token 251\n",
      "merging (14, 22) into a new token 252\n",
      "merging (12, 39) into a new token 253\n",
      "merging (8, 39) into a new token 254\n",
      "merging (94, 83) into a new token 255\n",
      "merging (49, 44) into a new token 256\n",
      "merging (7, 10) into a new token 257\n",
      "merging (47, 5) into a new token 258\n",
      "merging (47, 6) into a new token 259\n",
      "merging (10, 38) into a new token 260\n",
      "merging (13, 36) into a new token 261\n",
      "merging (21, 44) into a new token 262\n",
      "merging (32, 18) into a new token 263\n",
      "merging (16, 67) into a new token 264\n",
      "merging (5, 67) into a new token 265\n",
      "merging (16, 13) into a new token 266\n",
      "merging (61, 35) into a new token 267\n",
      "merging (6, 22) into a new token 268\n",
      "merging (8, 35) into a new token 269\n",
      "merging (11, 37) into a new token 270\n",
      "merging (8, 22) into a new token 271\n",
      "merging (16, 57) into a new token 272\n",
      "merging (50, 10) into a new token 273\n",
      "merging (26, 4) into a new token 274\n",
      "merging (73, 12) into a new token 275\n",
      "merging (20, 47) into a new token 276\n",
      "merging (39, 7) into a new token 277\n",
      "merging (34, 54) into a new token 278\n",
      "merging (16, 41) into a new token 279\n",
      "merging (4, 174) into a new token 280\n",
      "merging (38, 8) into a new token 281\n",
      "merging (21, 87) into a new token 282\n",
      "merging (40, 19) into a new token 283\n",
      "merging (14, 44) into a new token 284\n",
      "merging (5, 35) into a new token 285\n",
      "merging (6, 36) into a new token 286\n",
      "merging (18, 67) into a new token 287\n",
      "merging (72, 16) into a new token 288\n",
      "merging (55, 33) into a new token 289\n",
      "merging (22, 18) into a new token 290\n",
      "merging (33, 81) into a new token 291\n",
      "merging (5, 22) into a new token 292\n",
      "merging (58, 24) into a new token 293\n",
      "merging (49, 7) into a new token 294\n",
      "merging (40, 34) into a new token 295\n",
      "merging (25, 7) into a new token 296\n",
      "merging (51, 71) into a new token 297\n",
      "merging (108, 6) into a new token 298\n",
      "merging (42, 7) into a new token 299\n",
      "merging (13, 6) into a new token 61\n",
      "merging (15, 13) into a new token 62\n",
      "merging (18, 6) into a new token 63\n",
      "merging (23, 9) into a new token 64\n",
      "merging (28, 6) into a new token 65\n",
      "merging (14, 6) into a new token 66\n",
      "merging (16, 6) into a new token 67\n",
      "merging (13, 15) into a new token 68\n",
      "merging (31, 33) into a new token 69\n",
      "merging (21, 6) into a new token 70\n",
      "merging (18, 20) into a new token 71\n",
      "merging (26, 6) into a new token 72\n",
      "merging (25, 8) into a new token 73\n",
      "merging (31, 6) into a new token 74\n",
      "merging (11, 6) into a new token 75\n",
      "merging (61, 28) into a new token 76\n",
      "merging (18, 15) into a new token 77\n",
      "merging (10, 11) into a new token 78\n",
      "merging (9, 20) into a new token 79\n",
      "merging (42, 6) into a new token 80\n",
      "merging (42, 62) into a new token 81\n",
      "merging (20, 13) into a new token 82\n",
      "merging (13, 8) into a new token 83\n",
      "merging (20, 21) into a new token 84\n",
      "merging (18, 8) into a new token 85\n",
      "merging (21, 8) into a new token 86\n",
      "merging (13, 20) into a new token 87\n",
      "merging (28, 8) into a new token 88\n",
      "merging (14, 20) into a new token 89\n",
      "merging (28, 33) into a new token 90\n",
      "merging (31, 15) into a new token 91\n",
      "merging (61, 24) into a new token 92\n",
      "merging (7, 6) into a new token 93\n",
      "merging (26, 15) into a new token 94\n",
      "merging (11, 8) into a new token 95\n",
      "merging (9, 6) into a new token 96\n",
      "merging (23, 21) into a new token 97\n",
      "merging (33, 14) into a new token 98\n",
      "merging (64, 62) into a new token 99\n",
      "merging (28, 20) into a new token 100\n",
      "merging (31, 20) into a new token 101\n",
      "merging (28, 29) into a new token 102\n",
      "merging (13, 33) into a new token 103\n",
      "merging (25, 20) into a new token 104\n",
      "merging (31, 8) into a new token 105\n",
      "merging (28, 10) into a new token 106\n",
      "merging (24, 6) into a new token 107\n",
      "merging (11, 15) into a new token 108\n",
      "merging (26, 33) into a new token 109\n",
      "merging (36, 8) into a new token 110\n",
      "merging (26, 20) into a new token 111\n",
      "merging (5, 6) into a new token 112\n",
      "merging (7, 8) into a new token 113\n",
      "merging (9, 15) into a new token 114\n",
      "merging (21, 15) into a new token 115\n",
      "merging (9, 10) into a new token 116\n",
      "merging (26, 8) into a new token 117\n",
      "merging (23, 24) into a new token 118\n",
      "merging (21, 20) into a new token 119\n",
      "merging (11, 20) into a new token 120\n",
      "merging (80, 14) into a new token 121\n",
      "merging (18, 32) into a new token 122\n",
      "merging (28, 15) into a new token 123\n",
      "merging (14, 15) into a new token 124\n",
      "merging (13, 10) into a new token 125\n",
      "merging (9, 8) into a new token 126\n",
      "merging (66, 14) into a new token 127\n",
      "merging (79, 18) into a new token 128\n",
      "merging (27, 15) into a new token 129\n",
      "merging (31, 29) into a new token 130\n",
      "merging (36, 6) into a new token 131\n",
      "merging (24, 84) into a new token 132\n",
      "merging (77, 9) into a new token 133\n",
      "merging (64, 13) into a new token 134\n",
      "merging (8, 67) into a new token 135\n",
      "merging (23, 22) into a new token 136\n",
      "merging (7, 20) into a new token 137\n",
      "merging (22, 33) into a new token 138\n",
      "merging (13, 29) into a new token 139\n",
      "merging (25, 6) into a new token 140\n",
      "merging (25, 82) into a new token 141\n",
      "merging (18, 33) into a new token 142\n",
      "merging (13, 32) into a new token 143\n",
      "merging (28, 18) into a new token 144\n",
      "merging (42, 33) into a new token 145\n",
      "merging (114, 128) into a new token 146\n",
      "merging (133, 62) into a new token 147\n",
      "merging (7, 82) into a new token 148\n",
      "merging (42, 13) into a new token 149\n",
      "merging (24, 16) into a new token 150\n",
      "merging (42, 15) into a new token 151\n",
      "merging (62, 20) into a new token 152\n",
      "merging (26, 13) into a new token 153\n",
      "merging (10, 99) into a new token 154\n",
      "merging (94, 27) into a new token 155\n",
      "merging (11, 33) into a new token 156\n",
      "merging (30, 64) into a new token 157\n",
      "merging (18, 10) into a new token 158\n",
      "merging (36, 13) into a new token 159\n",
      "merging (28, 14) into a new token 160\n",
      "merging (24, 8) into a new token 161\n",
      "merging (79, 42) into a new token 162\n",
      "merging (7, 32) into a new token 163\n",
      "merging (14, 8) into a new token 164\n",
      "merging (35, 28) into a new token 165\n",
      "merging (116, 25) into a new token 166\n",
      "merging (24, 20) into a new token 167\n",
      "merging (31, 13) into a new token 168\n",
      "merging (7, 15) into a new token 169\n",
      "merging (30, 8) into a new token 170\n",
      "merging (23, 38) into a new token 171\n",
      "merging (22, 29) into a new token 172\n",
      "merging (25, 10) into a new token 173\n",
      "merging (9, 33) into a new token 174\n",
      "merging (26, 10) into a new token 175\n",
      "merging (36, 20) into a new token 176\n",
      "merging (18, 64) into a new token 177\n",
      "merging (63, 34) into a new token 178\n",
      "merging (7, 33) into a new token 179\n",
      "merging (5, 32) into a new token 180\n",
      "merging (12, 6) into a new token 181\n",
      "merging (22, 8) into a new token 182\n",
      "merging (36, 10) into a new token 183\n",
      "merging (11, 10) into a new token 184\n",
      "merging (37, 6) into a new token 185\n",
      "merging (22, 6) into a new token 186\n",
      "merging (45, 20) into a new token 187\n",
      "merging (81, 84) into a new token 188\n",
      "merging (5, 8) into a new token 189\n",
      "merging (19, 40) into a new token 190\n",
      "merging (42, 8) into a new token 191\n",
      "merging (14, 32) into a new token 192\n",
      "merging (37, 33) into a new token 193\n",
      "merging (31, 32) into a new token 194\n",
      "merging (24, 15) into a new token 195\n",
      "merging (26, 78) into a new token 196\n",
      "merging (18, 29) into a new token 197\n",
      "merging (11, 29) into a new token 198\n",
      "merging (15, 67) into a new token 199\n",
      "merging (12, 68) into a new token 200\n",
      "merging (26, 40) into a new token 201\n",
      "merging (24, 29) into a new token 202\n",
      "merging (72, 11) into a new token 203\n",
      "merging (14, 29) into a new token 204\n",
      "merging (65, 18) into a new token 205\n",
      "merging (21, 33) into a new token 206\n",
      "merging (31, 97) into a new token 207\n",
      "merging (24, 22) into a new token 208\n",
      "merging (24, 18) into a new token 209\n",
      "merging (7, 10) into a new token 210\n",
      "merging (102, 36) into a new token 211\n",
      "merging (21, 62) into a new token 212\n",
      "merging (22, 98) into a new token 213\n",
      "merging (11, 152) into a new token 214\n",
      "merging (7, 14) into a new token 215\n",
      "merging (14, 78) into a new token 216\n",
      "merging (88, 70) into a new token 217\n",
      "merging (24, 33) into a new token 218\n",
      "merging (13, 73) into a new token 219\n",
      "merging (24, 10) into a new token 220\n",
      "merging (24, 40) into a new token 221\n",
      "merging (87, 18) into a new token 222\n",
      "merging (35, 189) into a new token 223\n",
      "merging (35, 23) into a new token 224\n",
      "merging (14, 10) into a new token 225\n",
      "merging (42, 32) into a new token 226\n",
      "merging (12, 33) into a new token 227\n",
      "merging (36, 83) into a new token 228\n",
      "merging (26, 46) into a new token 229\n",
      "merging (28, 36) into a new token 230\n",
      "merging (31, 10) into a new token 231\n",
      "merging (68, 25) into a new token 232\n",
      "merging (35, 68) into a new token 233\n",
      "merging (11, 18) into a new token 234\n",
      "merging (13, 21) into a new token 235\n",
      "merging (116, 104) into a new token 236\n",
      "merging (13, 48) into a new token 237\n",
      "merging (31, 118) into a new token 238\n",
      "merging (48, 8) into a new token 239\n",
      "merging (15, 16) into a new token 240\n",
      "merging (36, 33) into a new token 241\n",
      "merging (24, 49) into a new token 242\n",
      "merging (26, 29) into a new token 243\n",
      "merging (27, 6) into a new token 244\n",
      "merging (115, 16) into a new token 245\n",
      "merging (28, 9) into a new token 246\n",
      "merging (100, 63) into a new token 247\n",
      "merging (35, 118) into a new token 248\n",
      "merging (38, 40) into a new token 249\n",
      "merging (23, 26) into a new token 250\n",
      "merging (200, 28) into a new token 251\n",
      "merging (16, 29) into a new token 252\n",
      "merging (28, 32) into a new token 253\n",
      "merging (106, 36) into a new token 254\n",
      "merging (90, 91) into a new token 255\n",
      "merging (13, 24) into a new token 256\n",
      "merging (31, 36) into a new token 257\n",
      "merging (155, 6) into a new token 258\n",
      "merging (23, 30) into a new token 259\n",
      "merging (35, 142) into a new token 260\n",
      "merging (48, 6) into a new token 261\n",
      "merging (31, 63) into a new token 262\n",
      "merging (109, 65) into a new token 263\n",
      "merging (70, 13) into a new token 264\n",
      "merging (30, 6) into a new token 265\n",
      "merging (97, 20) into a new token 266\n",
      "merging (45, 32) into a new token 267\n",
      "merging (22, 20) into a new token 268\n",
      "merging (45, 10) into a new token 269\n",
      "merging (61, 18) into a new token 270\n",
      "merging (138, 103) into a new token 271\n",
      "merging (18, 13) into a new token 272\n",
      "merging (31, 21) into a new token 273\n",
      "merging (143, 42) into a new token 274\n",
      "merging (69, 37) into a new token 275\n",
      "merging (14, 28) into a new token 276\n",
      "merging (19, 32) into a new token 277\n",
      "merging (21, 28) into a new token 278\n",
      "merging (18, 28) into a new token 279\n",
      "merging (42, 68) into a new token 280\n",
      "merging (19, 6) into a new token 281\n",
      "merging (21, 98) into a new token 282\n",
      "merging (108, 67) into a new token 283\n",
      "merging (24, 32) into a new token 284\n",
      "merging (85, 67) into a new token 285\n",
      "merging (68, 24) into a new token 286\n",
      "merging (25, 13) into a new token 287\n",
      "merging (21, 10) into a new token 288\n",
      "merging (64, 20) into a new token 289\n",
      "merging (21, 18) into a new token 290\n",
      "merging (30, 99) into a new token 291\n",
      "merging (55, 28) into a new token 292\n",
      "merging (145, 129) into a new token 293\n",
      "merging (74, 36) into a new token 294\n",
      "merging (11, 49) into a new token 295\n",
      "merging (12, 8) into a new token 296\n",
      "merging (126, 14) into a new token 297\n",
      "merging (14, 135) into a new token 298\n",
      "merging (13, 119) into a new token 299\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Now with the tokenizer class, initialize and train the tokenizers for processing the parallel corpus:\"\"\"\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "# BEGIN CODE : tokenizer.create\n",
    "\n",
    "# Initialize the tokenizers as per the desired strategy.\n",
    "# ADD YOUR CODE HERE\n",
    "src_tokenizer = Tokenizer()\n",
    "tgt_tokenizer = Tokenizer()\n",
    "\n",
    "# END CODE\n",
    "\n",
    "## ==== END EVALUATION PORTION\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "# Edit the hyperparameters below as desired.\n",
    "SRC_VOCAB_SIZE = 300\n",
    "TGT_VOCAB_SIZE = 300\n",
    "\n",
    "# BEGIN CODE : tokenizer.training\n",
    "\n",
    "# Train your tokenizer(s)\n",
    "# ADD YOUR CODE HERE\n",
    "train_data_src = train_data['Name'].tolist()\n",
    "train_data_tgt = train_data['Translation'].tolist()\n",
    "\n",
    "# print(train_data_src)\n",
    "\n",
    "# Train your tokenizer(s)\n",
    "src_tokenizer.train(train_data_src, SRC_VOCAB_SIZE)\n",
    "tgt_tokenizer.train(train_data_tgt, TGT_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "# END CODE\n",
    "\n",
    "## ==== END EVALUATION PORTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change anything in the following cell\n",
    "\n",
    "# Save the trained tokenizers\n",
    "src_tokenizer.save(os.path.join(DIRECTORY_NAME, \"src_tokenizer\"))\n",
    "tgt_tokenizer.save(os.path.join(DIRECTORY_NAME, \"tgt_tokenizer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change anything in the following cell\n",
    "\n",
    "def render_glyph(token):\n",
    "    \"\"\" Renders a token, handling invalid bytes in a safe, error-proof manner. \"\"\"\n",
    "\n",
    "    token = token.decode('utf-8', errors='replace') if isinstance(token, bytes) else token\n",
    "    return \"\".join([ c if unicodedata.category(c)[0] != \"C\" else f\"\\\\u{ord(c):04x}\" for c in token ])\n",
    "\n",
    "def inverse_vocabulary(tokenizer):\n",
    "    \"\"\" Generates an inverse vocabulary with rendered tokens.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (Tokenizer): Tokenizer whose vocabulary must be used.\n",
    "    \"\"\"\n",
    "\n",
    "    return { id: render_glyph(token) for token, id in tokenizer.get_vocabulary().items() }\n",
    "\n",
    "def apply_inverse_vocab(tokens, inv_vocab):\n",
    "    \"\"\" Decodes using the given inverse vocabulary.\n",
    "\n",
    "    Args:\n",
    "        tokens (list[int]): Tokens to process.\n",
    "        inv_vocab (dict[int, str]): Inverse vocabulary for mapping ids to tokens.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Mapped token glyphs.\n",
    "    \"\"\"\n",
    "\n",
    "    return [ inv_vocab[id] for id in tokens ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name           : prya\n",
      "Tokens         : [0, 94, 46, 1]\n",
      "Tokens (glyphs): ['<START>', 'pr', 'ya', '[EOS]']\n",
      "Decoded        : prya\n",
      "\n",
      "Name           : jamal\n",
      "Tokens         : [0, 187, 36, 1]\n",
      "Tokens (glyphs): ['<START>', 'jam', 'al', '[EOS]']\n",
      "Decoded        : jamal\n",
      "\n",
      "Name           : jag\n",
      "Tokens         : [0, 207, 1]\n",
      "Tokens (glyphs): ['<START>', 'jag', '[EOS]']\n",
      "Decoded        : jag\n",
      "\n",
      "Name           : sampat\n",
      "Tokens         : [0, 137, 76, 16, 1]\n",
      "Tokens (glyphs): ['<START>', 'sam', 'pa', 't', '[EOS]']\n",
      "Decoded        : sampat\n",
      "\n",
      "Name           : amar\n",
      "Tokens         : [0, 39, 35, 1]\n",
      "Tokens (glyphs): ['<START>', 'am', 'ar', '[EOS]']\n",
      "Decoded        : amar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We visualize a few outputs of the learnt tokenizers to assess their working:\"\"\"\n",
    "\n",
    "# Please do not change anything in the following cell\n",
    "\n",
    "src_id_to_token = inverse_vocabulary(src_tokenizer)\n",
    "\n",
    "for example in train_data['Name'].sample(n=5, random_state=20240227):\n",
    "    print(\"Name           :\", example)\n",
    "    tokens = src_tokenizer.encode(example)\n",
    "    print(\"Tokens         :\", tokens)\n",
    "    print(\"Tokens (glyphs):\", apply_inverse_vocab(tokens, src_id_to_token))\n",
    "    print(\"Decoded        :\", src_tokenizer.decode(tokens), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name           : प्र्या\n",
      "Tokens         : [0, 81, 199, 1]\n",
      "Tokens (glyphs): ['<START>', 'प्र', '्या', '[EOS]']\n",
      "Decoded        : प्र्या\n",
      "\n",
      "Name           : जमाल\n",
      "Tokens         : [0, 24, 65, 14, 1]\n",
      "Tokens (glyphs): ['<START>', 'ज', 'मा', 'ल', '[EOS]']\n",
      "Decoded        : जमाल\n",
      "\n",
      "Name           : जग\n",
      "Tokens         : [0, 208, 1]\n",
      "Tokens (glyphs): ['<START>', 'जग', '[EOS]']\n",
      "Decoded        : जग\n",
      "\n",
      "Name           : सम्पत्ति\n",
      "Tokens         : [0, 31, 123, 42, 115, 86, 1]\n",
      "Tokens (glyphs): ['<START>', 'स', 'म्', 'प', 'त्', 'ति', '[EOS]']\n",
      "Decoded        : सम्पत्ति\n",
      "\n",
      "Name           : अमर\n",
      "Tokens         : [0, 165, 13, 1]\n",
      "Tokens (glyphs): ['<START>', 'अम', 'र', '[EOS]']\n",
      "Decoded        : अमर\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change anything in the following cell\n",
    "\n",
    "tgt_id_to_token = inverse_vocabulary(tgt_tokenizer)\n",
    "\n",
    "for example in train_data['Translation'].sample(n=5, random_state=20240227):\n",
    "    print(\"Name           :\", example)\n",
    "    tokens = tgt_tokenizer.encode(example)\n",
    "    print(\"Tokens         :\", tokens)\n",
    "    print(\"Tokens (glyphs):\", apply_inverse_vocab(tokens, tgt_id_to_token))\n",
    "    print(\"Decoded        :\", tgt_tokenizer.decode(tokens), end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We now abstract away the tokenizer into a pytorch compatible TokenizedDataset that will handle the tokenization internally:'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please do not change anything in the following cell\n",
    "\n",
    "# Run some tests to ensure your tokenizer implementation works as intended.\n",
    "\n",
    "instances = train_data['Translation'].sample(n=5, random_state=20240227).tolist()\n",
    "\n",
    "try:\n",
    "    max_length = max(map(\n",
    "        lambda x: len(tgt_tokenizer.encode(x, add_start=False, add_end=False)),\n",
    "        instances\n",
    "    ))\n",
    "\n",
    "    # Batch encode all instances with 'max' padding':\n",
    "    tokenized_instances = tgt_tokenizer.batch_encode(\n",
    "        instances, padding=max_length, add_start=False, add_end=False\n",
    "    )\n",
    "\n",
    "    # Check if length of encoded strings is consistent with the expected length.\n",
    "    assert all(len(tok_str) == max_length for tok_str in tokenized_instances)\n",
    "\n",
    "except AssertionError:\n",
    "    print((\n",
    "        \"[!] Your tokenizer does not encode strings correctly, \"\n",
    "        \"ensure you have implemented padding appropriately!\"\n",
    "    ))\n",
    "\n",
    "try:\n",
    "    max_length = max(map(\n",
    "        lambda x: len(tgt_tokenizer.encode(x, add_start=True, add_end=True)),\n",
    "        instances\n",
    "    ))\n",
    "\n",
    "    # Batch encode all instances with 'max' padding':\n",
    "    tokenized_instances = tgt_tokenizer.batch_encode(\n",
    "        instances, padding=max_length, add_start=True, add_end=True\n",
    "    )\n",
    "\n",
    "    # Check if length of encoded strings is consistent with the expected length.\n",
    "    assert all(len(tok_str) == max_length for tok_str in tokenized_instances)\n",
    "\n",
    "    # # Check if all strings start with the correct 'start' tokens.\n",
    "    assert all(tok_str[0] == tokenized_instances[0][0] for tok_str in tokenized_instances)\n",
    "\n",
    "    # # Check if all strings end with the correct 'end' tokens.\n",
    "    end_i = [ i for i, seq in enumerate(tokenized_instances) if len(tgt_tokenizer.unpad(seq)) == max_length ]\n",
    "    pad_i = [ i for i, seq in enumerate(tokenized_instances) if len(tgt_tokenizer.unpad(seq)) <  max_length ]\n",
    "\n",
    "    assert all(\n",
    "        tokenized_instances[i][-1] == tokenized_instances[end_i[0]][-1]\n",
    "        for i in end_i\n",
    "    )\n",
    "    assert all(\n",
    "        tokenized_instances[i][-1] == tokenized_instances[pad_i[0]][-1]\n",
    "        for i in pad_i\n",
    "    )\n",
    "    pad_lengths = [ tokenized_instances[i].index(tokenized_instances[end_i[0]][-1]) for i in pad_i ]\n",
    "    assert all(\n",
    "        all(tok == tokenized_instances[pad_i[0]][-1] for tok in tokenized_instances[i][plen+1:])\n",
    "        for i, plen in zip(pad_i, pad_lengths)\n",
    "    )\n",
    "\n",
    "except AssertionError:\n",
    "    print((\n",
    "        \"[!] Your tokenizer does not encode strings correctly, \"\n",
    "        \"ensure you have used start and end tokens appropriately!\"\n",
    "    ))\n",
    "\n",
    "\"\"\"We now abstract away the tokenizer into a pytorch compatible TokenizedDataset that will handle the tokenization internally:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Please do not change anything in the following cell\n",
    "\n",
    "class TokenizerDataset(TensorDataset):\n",
    "    \"\"\" Abstraction of the tokenizer functions as a pytorch dataset. \"\"\"\n",
    "\n",
    "    def __init__(self, data, src_tokenizer, tgt_tokenizer, src_padding=None, tgt_padding=None):\n",
    "        \"\"\" Initializes the dataset.\n",
    "\n",
    "        Args:\n",
    "            data: DataFrame of input and output strings.\n",
    "            src_tokenizer (Tokenizer): Tokenizer for the source language.\n",
    "            tgt_tokenizer (Tokenizer): Tokenizer for the target language.\n",
    "            src_padding (int, optional): Padding length for the source text. Defaults to None.\n",
    "            tgt_padding (int, optional): Padding length for the target text. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = data\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.src_padding = src_padding\n",
    "        self.tgt_padding = tgt_padding\n",
    "\n",
    "    def collate(self, batch):\n",
    "        \"\"\" Collates data instances into a batch of tokenized tensors.\n",
    "\n",
    "        Args:\n",
    "            batch (list[tuple]): List of x, y pairs.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor|PackedSequence, torch.Tensor|PackedSequence]: pair of tokenized tensors.\n",
    "        \"\"\"\n",
    "\n",
    "        x_batch = [ data[0] for data in batch ]\n",
    "        y_batch = [ data[1] for data in batch ]\n",
    "\n",
    "        x_batch = self.src_tokenizer.batch_encode(x_batch, self.src_padding)\n",
    "        y_batch = self.tgt_tokenizer.batch_encode(y_batch, self.tgt_padding)\n",
    "\n",
    "        if self.src_padding is None:\n",
    "            x_batch = torch.nn.utils.rnn.pack_sequence([ torch.tensor(tokens) for tokens in x_batch ], False)\n",
    "        else:\n",
    "            x_batch = torch.tensor(x_batch)\n",
    "\n",
    "        if self.tgt_padding is None:\n",
    "            y_batch = torch.nn.utils.rnn.pack_sequence([ torch.tensor(tokens) for tokens in y_batch ], False)\n",
    "        else:\n",
    "            y_batch = torch.tensor(y_batch)\n",
    "\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Returns the nth instance from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the instance to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple[str, str]: Untokenized instance pair.\n",
    "        \"\"\"\n",
    "\n",
    "        return (\n",
    "            self.data['Name'][index],\n",
    "            self.data['Translation'][index]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Length of the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"## Model-Agnostic Training\n",
    "\n",
    "Next, you'll implement a Trainer to train different models, since the data and tokenizer remains the same for all models.\n",
    "\n",
    "This trainer will receive the model, a loss function, an optimizer, a training and (optionally) a validation dataset and use these to train (and validate) the model.\n",
    "\n",
    "The trainer will also take care of handling checkpoints for training, which can be used to resume training across sessions.\n",
    "\n",
    "Derived classes can also be defined to handle different architectures, as to be done in the model-specific classes below.\n",
    "\"\"\"\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\" Performs model training in a model-agnostic manner.\n",
    "        Requires specifying the model instance, the loss criterion to optimize,\n",
    "          the optimizer to use and the directory to save data to.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, directory, model, criterion, optimizer):\n",
    "        \"\"\" Initializes the trainer.\n",
    "\n",
    "        Args:\n",
    "            directory (str): Directory to save checkpoints and the model data in.\n",
    "            model (torch.nn.Module): Torch model (must inherit `torch.nn.Module`) to train.\n",
    "            criterion (torch.nn.Function): Loss criterion, i.e., the loss function to optimize for training.\n",
    "            optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model            = model\n",
    "        self.optimizer        = optimizer\n",
    "        self.criterion        = criterion\n",
    "        self.directory        = directory\n",
    "        self.last_checkpoint  = 0\n",
    "        self.loss_history     = { 'train': [], 'valid': [] }\n",
    "\n",
    "        os.makedirs(self.directory, exist_ok=True)\n",
    "        self.device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dataloader(dataset, shuffle_data=True, batch_size=8, collate_fn=None):\n",
    "        \"\"\" Create a dataloader for a torch Dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (torch.utils.data.Dataset): Dataset to process.\n",
    "            shuffle_data (bool, optional): If true, shuffles the data. Defaults to True.\n",
    "            batch_size (int, optional): Number of items per batch. Defaults to 8.\n",
    "            collate_fn (function, optional): Function to use for collating instances to a batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.utils.data.DataLoader: Dataloader over the given data, post processing.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : trainer.make_dataloader\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        return torch.utils.data.DataLoader(dataset, shuffle=shuffle_data, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def train_step(self, x_batch, y_batch):\n",
    "        \"\"\" Performs a step of training, on the training batch.\n",
    "\n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Input batch.\n",
    "            y_batch (torch.Tensor): Output batch.\n",
    "\n",
    "        Returns:\n",
    "            float: Training loss with the current model, on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : trainer.train_step\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "\n",
    "        outputs = self.model(x_batch)\n",
    "\n",
    "        loss = self.criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def eval_step(self, validation_dataloader):\n",
    "        \"\"\" Perfoms an evaluation step, on the validation dataloader.\n",
    "\n",
    "        Args:\n",
    "            validation_dataloader (torch.utils.data.DataLoader): Dataloader for the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            float: Validation loss with the current model checkpoint.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : trainer.eval_step\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in validation_dataloader:\n",
    "                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "                outputs = self.model(x_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        return total_loss / len(validation_dataloader)\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def train(self, train_dataset, validation_dataset=None,\n",
    "              num_epochs=10, batch_size=8, shuffle=True,\n",
    "              save_steps=100, eval_steps=100, collate_fn=None):\n",
    "        \"\"\" Handles the training loop for the model.\n",
    "\n",
    "        Args:\n",
    "            train_dataset (torch.utils.data.Dataset): Dataset to train on.\n",
    "            validation_dataset (torch.utils.data.Dataset, optional): Data to validate on. Defaults to None.\n",
    "            num_epochs (int, optional): Number of epochs to train for. Defaults to 10.\n",
    "            batch_size (int, optional): Number of items to process per batch. Defaults to 8.\n",
    "            shuffle (bool, optional): Whether to shuffle the data or not. Defaults to True.\n",
    "            save_steps (int, optional): Number of steps post which a checkpoint should be saved. Defaults to 100.\n",
    "            eval_steps (int, optional): Number of steps post which the model should be evaluated. Defaults to 100.\n",
    "            collate_fn (function, optional): Function to use for collating instances to a batch.\n",
    "        \"\"\"\n",
    "\n",
    "        current_checkpoint = 0\n",
    "        self.model.to(self.device)\n",
    "        self.model.train()\n",
    "\n",
    "        with tqdm.tqdm(total = math.ceil(len(train_dataset) / batch_size) * num_epochs) as pbar:\n",
    "            for epoch in range(num_epochs):\n",
    "                train_dataloader      = self.make_dataloader(train_dataset, shuffle, batch_size, collate_fn)\n",
    "                if validation_dataset is not None:\n",
    "                    validation_dataloader = self.make_dataloader(validation_dataset, shuffle, batch_size, collate_fn)\n",
    "\n",
    "                for batch, (x_batch, y_batch) in enumerate(train_dataloader):\n",
    "                    pbar.set_description(f\"Epoch {epoch+1} / {num_epochs}\")\n",
    "\n",
    "                    # If we are resuming training, skip this iteration\n",
    "                    if current_checkpoint < self.last_checkpoint:\n",
    "                        current_checkpoint += 1\n",
    "                        pbar.update()\n",
    "                        continue\n",
    "\n",
    "                    # Do a step of training\n",
    "                    loss = self.train_step(x_batch, y_batch)\n",
    "                    self.loss_history['train'].append(loss)\n",
    "                    pbar.set_postfix({ 'batch': batch+1, 'loss': loss })\n",
    "\n",
    "                    current_checkpoint += 1\n",
    "                    pbar.update()\n",
    "\n",
    "                    # Evaluate after every eval_steps\n",
    "                    if (current_checkpoint) % eval_steps == 0:\n",
    "                        if validation_dataset is not None:\n",
    "                            val_loss = self.eval_step(validation_dataloader)\n",
    "                            self.loss_history['valid'].append(val_loss)\n",
    "                        else:\n",
    "                            val_loss = None\n",
    "\n",
    "                        print('[>]', f\"epoch #{epoch+1:{len(str(num_epochs))}},\",\n",
    "                              f\"batch #{batch+1:{len(str(len(train_dataloader)))}}:\",\n",
    "                              \"loss:\", f\"{loss:.8f}\", '|', \"val_loss:\", f\"{val_loss:.8f}\")\n",
    "\n",
    "                    # Save after every save_steps\n",
    "                    if (current_checkpoint) % save_steps == 0:\n",
    "                        self.save(current_checkpoint, { 'loss': loss, 'checkpoint': current_checkpoint })\n",
    "\n",
    "                    # free unused resources\n",
    "                    sync_vram()\n",
    "\n",
    "            self.save(current_checkpoint)\n",
    "\n",
    "    def resume(self):\n",
    "        \"\"\" Resumes training session from the most recent checkpoint. \"\"\"\n",
    "\n",
    "        if checkpoints := os.listdir(self.directory):\n",
    "            self.last_checkpoint = max(map(lambda x: int(x[11:]), filter(lambda x: 'checkpoint-' in x, checkpoints)))\n",
    "            checkpoint_dir = os.path.join(self.directory, f\"checkpoint-{self.last_checkpoint}\")\n",
    "            self.model.load_state_dict(torch.load(\n",
    "                os.path.join(checkpoint_dir, \"model.pt\"),\n",
    "                map_location=self.device\n",
    "            ))\n",
    "            self.model.to(self.device)\n",
    "            self.optimizer.load_state_dict(torch.load(\n",
    "                os.path.join(checkpoint_dir, \"optimizer.pt\"),\n",
    "                map_location=self.device\n",
    "            ))\n",
    "            with open(os.path.join(checkpoint_dir, \"loss.json\"), 'r', encoding='utf-8') as ifile:\n",
    "                self.loss_history = json.load(ifile)\n",
    "\n",
    "    def save(self, checkpoint=None, metadata=None):\n",
    "        \"\"\" Saves an associated model or a training checkpoint.\n",
    "\n",
    "            If a checkpoint is specified, saves a checkpoint specific directory with optimizer data\n",
    "                so that training can be resumed post that checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint (int, optional): Checkpoint index. Defaults to None.\n",
    "            metadata (dict[str, any], optional): Additional metadata to save alongside a checkpoint. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        if checkpoint is not None:\n",
    "            checkpoint_dir = os.path.join(self.directory, f\"checkpoint-{checkpoint}\")\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            torch.save(self.model.state_dict(), os.path.join(checkpoint_dir, \"model.pt\"))\n",
    "            torch.save(self.optimizer.state_dict(), os.path.join(checkpoint_dir, \"optimizer.pt\"))\n",
    "            with open(os.path.join(checkpoint_dir, \"loss.json\"), \"w+\", encoding='utf-8') as ofile:\n",
    "                json.dump(self.loss_history, ofile, ensure_ascii=False, indent=2)\n",
    "            if metadata:\n",
    "                with open(os.path.join(checkpoint_dir, \"metadata.json\"), \"w+\", encoding='utf-8') as ofile:\n",
    "                    json.dump(metadata, ofile, ensure_ascii=False, indent=2)\n",
    "        else:\n",
    "            torch.save(self.model, os.path.join(self.directory, \"model.pt\"))\n",
    "            with open(os.path.join(self.directory, \"loss.json\"), \"w+\", encoding='utf-8') as ofile:\n",
    "                json.dump(self.loss_history, ofile, ensure_ascii=False, indent=2)\n",
    "            if metadata:\n",
    "                with open(os.path.join(self.directory, \"metadata.json\"), \"w+\", encoding='utf-8') as ofile:\n",
    "                    json.dump(metadata, ofile, ensure_ascii=False, indent=2)\n",
    "\n",
    "## ==== END EVALUATION PORTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 10: 100%|██████████| 10/10 [00:03<00:00,  3.21it/s, batch=1, loss=0.157]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "To test that the trainer works, try training a simple MLP network:\"\"\"\n",
    "\n",
    "X_train = torch.rand((500, 2))                      # (N x 2)\n",
    "X_dev   = torch.rand((20 , 2))                      # (N x 2)\n",
    "\n",
    "Y_train = (X_train[:, 0] - X_train[:, 1])[:, None]  # (N x 1)\n",
    "Y_dev   = (X_dev  [:, 0] - X_dev  [:, 1])[:, None]  # (N x 1)\n",
    "\n",
    "dummy_train_dataset = TensorDataset(X_train, Y_train)\n",
    "dummy_val_dataset   = TensorDataset(X_dev  , Y_dev  )\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4, 1)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "trainer = Trainer(\"mlp\", model, loss_fn, optimizer)\n",
    "trainer.train(dummy_train_dataset, dummy_val_dataset, batch_size=512, save_steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Seq-2-Seq Modeling with RNNs\n",
    "\n",
    "In this section, you will implement an encoder-decoder network using RNNs, to learn a conditional language model for the task of translating the names to Hindi.\n",
    "\n",
    "You can use any type of RNN for this purpose: `RNN`, `GRU`, `LSTM`, etc. Consult the pytorch documentation for additional information.\n",
    "\n",
    "Additional tips for training:\n",
    "- Use regularization: Dropout, etc.\n",
    "- Use a suitable optimizer, such as Adam.\n",
    "- Format data accordingly before passing it to the trainer, using the helper functions.\n",
    "- Do you need to pad sequences when processing inputs as a batch?\n",
    "\"\"\"\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "class RNNEncoderDecoderLM(torch.nn.Module):\n",
    "    \"\"\" Implements an Encoder-Decoder network, using RNN units. \"\"\"\n",
    "\n",
    "    # Feel free to add additional parameters to __init__\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embd_dims, hidden_size, num_layers=1, dropout=0.1):\n",
    "        \"\"\" Initializes the encoder-decoder network, implemented via RNNs.\n",
    "\n",
    "        Args:\n",
    "            src_vocab_size (int): Source vocabulary size.\n",
    "            tgt_vocab_size (int): Target vocabulary size.\n",
    "            embd_dims (int): Embedding dimensions.\n",
    "            hidden_size (int): Size/Dimensions for the hidden states.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNEncoderDecoderLM, self).__init__()\n",
    "\n",
    "        # Dummy parameter to track the model device. Do not modify.\n",
    "        self._dummy_param = torch.nn.Parameter(torch.Tensor(0), requires_grad=False)\n",
    "\n",
    "        # BEGIN CODE : enc-dec-rnn.init\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        self.src_vocab_size  = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.embd_dims = embd_dims\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "         # Define encoder\n",
    "        self.encoder = nn.Embedding(src_vocab_size, embd_dims)\n",
    "        self.encoder_rnn = nn.GRU(embd_dims, hidden_size, num_layers,batch_first = True, dropout=dropout)\n",
    "\n",
    "        # Define decoder\n",
    "        self.decoder = nn.Embedding(tgt_vocab_size, embd_dims)\n",
    "        self.decoder_rnn = nn.GRU(embd_dims, hidden_size, num_layers,batch_first = True, dropout=dropout)\n",
    "        self.decoder_fc = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\" Returns the device the model parameters are on. \"\"\"\n",
    "        return self._dummy_param.device\n",
    "\n",
    "    def forward(self, inputs, decoder_inputs, decoder_hidden_state=None):\n",
    "        \"\"\" Performs a forward pass over the encoder-decoder network.\n",
    "\n",
    "            Accepts inputs for the encoder, inputs for the decoder, and hidden state for\n",
    "                the decoder to continue generation after the given input.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): tensor of shape [batch_size?, max_seq_length]\n",
    "            decoder_inputs (torch.Tensor): tensor of shape [batch_size?, 1]\n",
    "            decoder_hidden_state (any): tensor to represent decoder hidden state from time step T-1.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, any]: output from the decoder, and associated hidden state for the next step.\n",
    "            Decoder outputs should be log probabilities over the target vocabulary.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : enc-dec-rnn.forward\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        if(decoder_hidden_state == None):\n",
    "          encoder_embedded = self.dropout_layer(self.encoder(inputs))\n",
    "          encoder_outputs, encoder_hidden = self.encoder_rnn(encoder_embedded)\n",
    "          decoder_hidden_state = encoder_hidden\n",
    "\n",
    "\n",
    "        decoder_embedded = self.dropout_layer(self.decoder(decoder_inputs))\n",
    "\n",
    "        decoder_outputs, decoder_hidden = self.decoder_rnn(decoder_embedded, decoder_hidden_state)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.decoder_fc(decoder_outputs)\n",
    "\n",
    "        output_probs = torch.nn.functional.log_softmax(output,dim=-1)\n",
    "\n",
    "        return output_probs, decoder_hidden\n",
    "\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def log_probability(self, seq_x, seq_y):\n",
    "        \"\"\" Compute the conditional log probability of seq_y given seq_x, i.e., log P(seq_y | seq_x).\n",
    "\n",
    "        Args:\n",
    "            seq_x (torch.tensor): Input sequence of tokens.\n",
    "            seq_y (torch.tensor): Output sequence of tokens.\n",
    "\n",
    "        Returns:\n",
    "            float: Log probability of seq_y given seq_x\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : enc-dec-rnn.log_probability\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "## ==== END EVALUATION PORTION\n",
    "\n",
    "\"\"\"To train the above model, implement for training and evaluation steps in the `RNNEncoderDecoderTrainer` class below:\"\"\"\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "class RNNEncoderDecoderTrainer(Trainer):\n",
    "    \"\"\" Performs model training for RNN-based Encoder-Decoder models. \"\"\"\n",
    "\n",
    "    def __init__(self, directory, model, criterion, optimizer):\n",
    "        \"\"\" Initializes the trainer.\n",
    "\n",
    "        Args:\n",
    "            directory (str): Directory to save checkpoints and the model data in.\n",
    "            model (torch.nn.Module): Torch model to train.\n",
    "            criterion (torch.nn.Function): Loss Criterion.\n",
    "            optimizer (torch.optim.Optimizer): Optimizer to use.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNEncoderDecoderTrainer, self).__init__(directory, model, criterion, optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def make_dataloader(dataset, shuffle_data=True, batch_size=8, collate_fn=None):\n",
    "        \"\"\" Create a dataloader for a torch Dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (torch.utils.data.Dataset): Dataset to process.\n",
    "            shuffle_data (bool, optional): If true, shuffles the data. Defaults to True.\n",
    "            batch_size (int, optional): Number of items per batch. Defaults to 8.\n",
    "            collate_fn (function, optional): Function to collate instances in a batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.utils.data.DataLoader: Dataloader over the given data, post processing.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : rnn-enc-dec-trainer.make_dataloader\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        return torch.utils.data.DataLoader(dataset, shuffle=shuffle_data, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def train_step(self, x_batch, y_batch):\n",
    "        \"\"\" Performs a step of training, on the training batch.\n",
    "\n",
    "        Args:\n",
    "            x_batch (torch.Tensor): Input batch tensor, of shape [batch_size, *instance_shape].\n",
    "              For RNNs this is [batch_size, src_padding] or a torch.nn.utils.rnn.PackedSequence of varying lengths per batch (depends on padding).\n",
    "            y_batch (torch.Tensor): Output batch tensor, of shape [batch_size, *instance_shape].\n",
    "              For RNNs this is [batch_size, tgt_padding] or a torch.nn.utils.rnn.PackedSequence of varying lengths per batch (depends on padding).\n",
    "\n",
    "        Returns:\n",
    "            float: Training loss with the current model, on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : rnn-enc-dec-trainer.train_step\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        x_batch = x_batch.to(self.device)\n",
    "        y_batch = y_batch.to(self.device)\n",
    "\n",
    "\n",
    "        loss = 0\n",
    "        self.model.train()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        n = y_batch.shape[1]\n",
    "        decoder_hidden_state = None\n",
    "\n",
    "        for i in range(n-1):\n",
    "          output,decoder_hidden_state = self.model(x_batch,y_batch[:,i:i+1],decoder_hidden_state)\n",
    "\n",
    "          loss += self.criterion(output.squeeze(1),y_batch[:,i+1:i+2].squeeze(1))\n",
    "\n",
    "        loss = loss/n\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def eval_step(self, validation_dataloader):\n",
    "        \"\"\" Perfoms an evaluation step, on the validation dataloader.\n",
    "\n",
    "        Args:\n",
    "            validation_dataloader (torch.utils.data.DataLoader): Dataloader for the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            float: Validation loss with the current model checkpoint.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : rnn-enc-dec-trainer.eval_step\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x_data, y_data in validation_dataloader:\n",
    "\n",
    "                x_data = x_data.to(self.device)\n",
    "                y_data = y_data.to(self.device)\n",
    "\n",
    "                decoder_hidden_state = None\n",
    "\n",
    "                n = y_data.shape[1]\n",
    "                loss = 0\n",
    "\n",
    "                for i in range(n-1):\n",
    "                  output, decoder_hidden_state = self.model(x_data,y_data[:,i:i+1],decoder_hidden_state)\n",
    "                  loss += self.criterion(output.squeeze(1),y_data[:,i+1:i+2].squeeze(1))\n",
    "\n",
    "                total_loss += loss.item()/n\n",
    "\n",
    "\n",
    "        return total_loss / len(validation_dataloader)\n",
    "\n",
    "        # END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==== END EVALUATION PORTION\n",
    "\n",
    "## == BEGIN EVALUATION PORTION\n",
    "\n",
    "# Edit the hyperparameters below to your desired values.\n",
    "\n",
    "# BEGIN CODE : rnn-enc-dec.params\n",
    "\n",
    "# Add parameters related to the model here.\n",
    "rnn_enc_dec_params = {\n",
    "    'src_vocab_size': SRC_VOCAB_SIZE+4,\n",
    "    'tgt_vocab_size': TGT_VOCAB_SIZE+4,\n",
    "    'embd_dims'     : 300,\n",
    "    'hidden_size'   : 512,\n",
    "    'dropout'       : 0.1,\n",
    "    'num_layers'    : 2\n",
    "}\n",
    "\n",
    "# Add parameters related to the dataset processing here.\n",
    "rnn_enc_dec_data_params = dict(\n",
    "    src_padding=18,\n",
    "    tgt_padding=18,\n",
    ")\n",
    "\n",
    "# Add parameters related to training here.\n",
    "rnn_enc_dec_training_params = dict(\n",
    "    num_epochs=50,\n",
    "    batch_size=130,\n",
    "    shuffle=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=50\n",
    ")\n",
    "\n",
    "# END CODE\n",
    "\n",
    "# Do not forget to set a deterministic seed.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = RNNEncoderDecoderLM(**rnn_enc_dec_params)\n",
    "\n",
    "# BEGIN CODE : rnn-enc-dec.train\n",
    "\n",
    "# ADD YOUR CODE HERE\n",
    "optimizer = optim.Adam(model.parameters(),lr =0.0023)#0.003\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RNNEncoderDecoderTrainer(\n",
    "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec\"),\n",
    "    model, criterion, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 / 50:   3%|▎         | 51/1750 [00:23<14:38,  1.93it/s, batch=16, loss=0.877]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch # 2, batch #15: loss: 0.91028959 | val_loss: 0.86324901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 / 50:   6%|▌         | 100/1750 [00:41<08:52,  3.10it/s, batch=30, loss=0.62]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch # 3, batch #30: loss: 0.61974317 | val_loss: 0.62733164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 / 50:   9%|▊         | 151/1750 [00:58<10:08,  2.63it/s, batch=11, loss=0.339]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch # 5, batch #10: loss: 0.28939906 | val_loss: 0.39998043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 / 50:  11%|█▏        | 200/1750 [01:14<08:34,  3.02it/s, batch=25, loss=0.218]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch # 6, batch #25: loss: 0.21804689 | val_loss: 0.33132210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 / 50:  14%|█▍        | 250/1750 [01:30<07:32,  3.31it/s, batch=5, loss=0.0862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch # 8, batch # 5: loss: 0.08618800 | val_loss: 0.29747427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 / 50:  17%|█▋        | 300/1750 [01:48<10:22,  2.33it/s, batch=20, loss=0.0637]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch # 9, batch #20: loss: 0.06367382 | val_loss: 0.27796090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 / 50:  20%|██        | 350/1750 [02:07<27:27,  1.18s/it, batch=35, loss=0.0365]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #10, batch #35: loss: 0.03649116 | val_loss: 0.29477294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 / 50:  23%|██▎       | 400/1750 [02:24<06:24,  3.51it/s, batch=15, loss=0.0222]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #12, batch #15: loss: 0.02222405 | val_loss: 0.26698298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 / 50:  26%|██▌       | 450/1750 [02:41<06:48,  3.18it/s, batch=30, loss=0.0133] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #13, batch #30: loss: 0.01333435 | val_loss: 0.29709550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 / 50:  29%|██▊       | 500/1750 [02:57<06:55,  3.01it/s, batch=10, loss=0.00601]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #15, batch #10: loss: 0.00601299 | val_loss: 0.31783173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 / 50:  31%|███▏      | 550/1750 [03:19<06:03,  3.30it/s, batch=25, loss=0.00531]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #16, batch #25: loss: 0.00530609 | val_loss: 0.29960971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 / 50:  34%|███▍      | 600/1750 [03:35<05:38,  3.40it/s, batch=5, loss=0.0086]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #18, batch # 5: loss: 0.00859608 | val_loss: 0.28070081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 / 50:  37%|███▋      | 651/1750 [03:52<05:41,  3.22it/s, batch=21, loss=0.00442]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #19, batch #20: loss: 0.00718028 | val_loss: 0.31962972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 / 50:  40%|████      | 700/1750 [04:08<05:33,  3.15it/s, batch=35, loss=0.00252]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #20, batch #35: loss: 0.00251885 | val_loss: 0.31762057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 / 50:  43%|████▎     | 751/1750 [04:26<05:14,  3.18it/s, batch=16, loss=0.00187]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #22, batch #15: loss: 0.00212520 | val_loss: 0.30453631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 / 50:  46%|████▌     | 800/1750 [04:40<05:05,  3.11it/s, batch=30, loss=0.00192]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #23, batch #30: loss: 0.00191550 | val_loss: 0.28491279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 / 50:  49%|████▊     | 850/1750 [05:00<05:42,  2.63it/s, batch=10, loss=0.00152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #25, batch #10: loss: 0.00152162 | val_loss: 0.30662446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 / 50:  51%|█████▏    | 900/1750 [05:15<03:59,  3.55it/s, batch=25, loss=0.00154]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #26, batch #25: loss: 0.00154075 | val_loss: 0.32144154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 / 50:  54%|█████▍    | 950/1750 [05:32<04:25,  3.01it/s, batch=5, loss=0.00146] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #28, batch # 5: loss: 0.00146067 | val_loss: 0.29540367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 / 50:  57%|█████▋    | 1000/1750 [05:50<04:46,  2.61it/s, batch=20, loss=0.00592]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #29, batch #20: loss: 0.00592487 | val_loss: 0.29812826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 / 50:  60%|██████    | 1050/1750 [06:08<04:15,  2.74it/s, batch=35, loss=0.00371]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #30, batch #35: loss: 0.00371187 | val_loss: 0.30440325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 / 50:  63%|██████▎   | 1100/1750 [06:25<03:43,  2.91it/s, batch=15, loss=0.00228]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #32, batch #15: loss: 0.00228291 | val_loss: 0.37815235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 / 50:  66%|██████▌   | 1150/1750 [06:47<03:21,  2.98it/s, batch=30, loss=0.00444]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #33, batch #30: loss: 0.00444321 | val_loss: 0.38358985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 / 50:  69%|██████▊   | 1200/1750 [07:03<02:50,  3.23it/s, batch=10, loss=0.00264]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #35, batch #10: loss: 0.00264438 | val_loss: 0.33617902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 / 50:  71%|███████▏  | 1250/1750 [07:21<02:42,  3.08it/s, batch=25, loss=0.00571]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #36, batch #25: loss: 0.00571077 | val_loss: 0.30230435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 / 50:  74%|███████▍  | 1300/1750 [07:37<02:28,  3.04it/s, batch=5, loss=0.0118]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #38, batch # 5: loss: 0.01177827 | val_loss: 0.35211503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 / 50:  77%|███████▋  | 1351/1750 [07:54<02:07,  3.12it/s, batch=21, loss=0.0403]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #39, batch #20: loss: 0.02892320 | val_loss: 0.37999917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 / 50:  80%|████████  | 1400/1750 [08:12<05:59,  1.03s/it, batch=35, loss=0.0389]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #40, batch #35: loss: 0.03885196 | val_loss: 0.32594474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 / 50:  83%|████████▎ | 1451/1750 [08:31<01:42,  2.91it/s, batch=16, loss=0.0271]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #42, batch #15: loss: 0.03508841 | val_loss: 0.34787748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 / 50:  86%|████████▌ | 1500/1750 [08:47<01:18,  3.18it/s, batch=30, loss=0.0177]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #43, batch #30: loss: 0.01768797 | val_loss: 0.32443039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 / 50:  89%|████████▊ | 1550/1750 [09:08<02:03,  1.63it/s, batch=10, loss=0.0066] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #45, batch #10: loss: 0.00660139 | val_loss: 0.38398136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 / 50:  91%|█████████▏| 1600/1750 [09:25<00:44,  3.34it/s, batch=25, loss=0.00356]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #46, batch #25: loss: 0.00355891 | val_loss: 0.36011540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 / 50:  94%|█████████▍| 1650/1750 [09:43<00:40,  2.49it/s, batch=5, loss=0.00114] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #48, batch # 5: loss: 0.00114435 | val_loss: 0.31017096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 / 50:  97%|█████████▋| 1700/1750 [10:00<00:21,  2.33it/s, batch=20, loss=0.000751]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #49, batch #20: loss: 0.00075126 | val_loss: 0.31693081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 / 50: 100%|██████████| 1750/1750 [10:16<00:00,  3.33it/s, batch=35, loss=0.000617]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #50, batch #35: loss: 0.00061712 | val_loss: 0.38688460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 / 50: 100%|██████████| 1750/1750 [10:17<00:00,  2.84it/s, batch=35, loss=0.000617]\n"
     ]
    }
   ],
   "source": [
    "## == END EVALUATION PORTION\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "train_dataset      = TokenizerDataset(train_data     , src_tokenizer, tgt_tokenizer, **rnn_enc_dec_data_params)\n",
    "validation_dataset = TokenizerDataset(validation_data, src_tokenizer, tgt_tokenizer, **rnn_enc_dec_data_params)\n",
    "\n",
    "rnn_enc_dec_train_data = dict(\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    collate_fn=train_dataset.collate\n",
    ")\n",
    "\n",
    "# Resume training from the last checkpoint, if interrupted midway, else begins training from scratch.\n",
    "#trainer.resume()\n",
    "\n",
    "# Train as per specified training parameters.\n",
    "trainer.train(**rnn_enc_dec_train_data, **rnn_enc_dec_training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "# Save the final model, with additional metadata.\n",
    "trainer.save(metadata={\n",
    "    'model'   : rnn_enc_dec_params,\n",
    "    'data'    : rnn_enc_dec_data_params,\n",
    "    'training': rnn_enc_dec_training_params\n",
    "})\n",
    "\n",
    "\"\"\"To validate training, look at sample translations for different examples, and probabilities assigned to different outputs.\n",
    "\n",
    "Extensive evaluation and comparison against other approaches will be carried out later.\n",
    "\"\"\"\n",
    "\n",
    "device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "device\n",
    "\n",
    "def rnn_greedy_generate(model, seq_x, src_tokenizer, tgt_tokenizer, max_length):\n",
    "    \"\"\" Given a source string, translate it to the target language using the trained model.\n",
    "        This function should perform greedy sampling to generate the results.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): RNN Type Encoder-Decoder Model\n",
    "        seq_x (str): Input string to translate.\n",
    "        src_tokenizer (Tokenizer): Source language tokenizer.\n",
    "        tgt_tokenizer (Tokenizer): Target language tokenizer.\n",
    "        max_length (int): Maximum length of the target sequence to decode.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated string for the given input in the target language.\n",
    "    \"\"\"\n",
    "\n",
    "    # BEGIN CODE : enc-dec-rnn.greedy_generate\n",
    "\n",
    "    # ADD YOUR CODE HERE\n",
    "    seq_x_indices = src_tokenizer.encode(seq_x, add_start=True, add_end=True)\n",
    "\n",
    "    seq_x_tensor = torch.tensor(seq_x_indices).unsqueeze(0)\n",
    "\n",
    "    seq_x_tensor = seq_x_tensor.to(device)\n",
    "\n",
    "    top_token_index  = [tgt_tokenizer.token_to_id['<START>']]\n",
    "\n",
    "    decoder_input = torch.tensor(top_token_index)\n",
    "\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    decoder_hidden_state = None\n",
    "\n",
    "    tgt_sequence = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(max_length):\n",
    "\n",
    "\n",
    "            output_probs, decoder_hidden_state = model(seq_x_tensor, decoder_input.unsqueeze(0),decoder_hidden_state)\n",
    "\n",
    "            top_token_index = output_probs.argmax(dim = -1)[:,-1]\n",
    "            decoder_input = top_token_index\n",
    "            tgt_sequence.append(top_token_index.item())\n",
    "\n",
    "            if top_token_index[0] == tgt_tokenizer.token_to_id['[EOS]'] or len(tgt_sequence) >= max_length:\n",
    "                break\n",
    "\n",
    "\n",
    "    tgt_sequence.insert(0,tgt_tokenizer.token_to_id['<START>'])\n",
    "\n",
    "\n",
    "    # print(tgt_sequence)\n",
    "\n",
    "    # tgt_tokens = [tgt_tokenizer.id_to_token[idx] for idx in tgt_sequence[1:]]  # Exclude <SOS>\n",
    "\n",
    "    return tgt_tokenizer.decode(tgt_sequence)\n",
    "    # return generated_string\n",
    "\n",
    "    # END CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                      : al0k\n",
      "Translation (Expected)    : आलोक\n",
      "Translation (Model)       : आलोक\n",
      "\n",
      "Name                      : sufia\n",
      "Translation (Expected)    : सूफिया\n",
      "Translation (Model)       : सूया\n",
      "\n",
      "Name                      : firoz\n",
      "Translation (Expected)    : फिरोज़\n",
      "Translation (Model)       : फित\n",
      "\n",
      "Name                      : gouri\n",
      "Translation (Expected)    : गौरी\n",
      "Translation (Model)       : गौरी\n",
      "\n",
      "Name                      : chhaya\n",
      "Translation (Expected)    : छाया\n",
      "Translation (Model)       : छा\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change anything in the following cell.\n",
    "\n",
    "for _, row in train_data.sample(n=5, random_state=42).iterrows():\n",
    "    y_pred = rnn_greedy_generate(\n",
    "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
    "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
    "    )\n",
    "\n",
    "    print(\"Name                      :\", row['Name'])\n",
    "    print(\"Translation (Expected)    :\", row['Translation'])\n",
    "    print(\"Translation (Model)       :\", y_pred)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                      : mhosin\n",
      "Translation (Expected)    : मोहसीन\n",
      "Translation (Model)       : एमएचओ\n",
      "\n",
      "Name                      : qadeem\n",
      "Translation (Expected)    : क़दीम\n",
      "Translation (Model)       : क़ाम\n",
      "\n",
      "Name                      : ashiqu\n",
      "Translation (Expected)    : आशिक़\n",
      "Translation (Model)       : आशिक\n",
      "\n",
      "Name                      : midhana\n",
      "Translation (Expected)    : मिधना\n",
      "Translation (Model)       : मिना\n",
      "\n",
      "Name                      : divakar\n",
      "Translation (Expected)    : दिवाकर\n",
      "Translation (Model)       : दिवार\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change anything in the following cell.\n",
    "\n",
    "for _, row in validation_data.sample(n=5, random_state=42).iterrows():\n",
    "    y_pred = rnn_greedy_generate(\n",
    "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
    "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
    "    )\n",
    "\n",
    "    print(\"Name                      :\", row['Name'])\n",
    "    print(\"Translation (Expected)    :\", row['Translation'])\n",
    "    print(\"Translation (Model)       :\", y_pred)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change anything in the following cell.\n",
    "\n",
    "output_data = []\n",
    "for _, row in validation_data.iterrows():\n",
    "    y_pred = rnn_greedy_generate(\n",
    "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
    "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
    "    )\n",
    "    output_data.append({ 'Name': row['Name'], 'Translation': y_pred })\n",
    "\n",
    "pd.DataFrame.from_records(output_data).to_csv(\n",
    "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec\", \"outputs.csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change anything in the following cell.\n",
    "\n",
    "# Release resources\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "\n",
    "sync_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Seq-2-Seq Modeling with RNN + Attention\n",
    "\n",
    "In this module, you'll augment the Encoder-Decoder architecture to utilize attention, by implementing an Attention module that attends over the representations / inputs from the encoder.\n",
    "\n",
    "Many approaches have been proposed in literature towards implementing attention. You are free to explore and use any implementation of your choice.\n",
    "\n",
    "Some popular approaches are desribed in the original [paper by Bahdanau et al., 2014 on NMT](https://arxiv.org/abs/1409.0473) and an [exploratory paper by Luong et al, 2015](https://arxiv.org/abs/1508.04025) which explores different effective approaches to attention, including global and local attention.\n",
    "\"\"\"\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "class AttentionModule(torch.nn.Module):\n",
    "    \"\"\" Implements an attention module \"\"\"\n",
    "\n",
    "    # Feel free to add additional parameters to __init__\n",
    "    def __init__(self, input_size):\n",
    "        \"\"\" Initializes the attention module.\n",
    "            Feel free to declare any parameters as required. \"\"\"\n",
    "\n",
    "        super(AttentionModule, self).__init__()\n",
    "\n",
    "        # BEGIN CODE : attn.init\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        self.Wa = nn.Linear(input_size, input_size)\n",
    "        self.Ua = nn.Linear(input_size, input_size)\n",
    "        self.Va = nn.Linear(input_size, 1)\n",
    "\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden_state):\n",
    "        \"\"\" Performs a forward pass over the module, computing attention scores for inputs.\n",
    "\n",
    "        Args:\n",
    "            encoder_outputs (torch.Tensor): Output representations from the encoder, of shape [batch_size?, src_seq_len, output_dim].\n",
    "            decoder_hidden_state (torch.Tensor): Hidden state from the decoder at current time step, of appropriate shape as per RNN unit (with optional batch dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Attentions scores for given inputs, of shape [batch_size?, 1, src_seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : attn.forward\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "\n",
    "\n",
    "        U = self.Ua(decoder_hidden_state)\n",
    "        W = self.Wa(encoder_outputs)\n",
    "        scores = self.Va(torch.tanh(W +U ))\n",
    "\n",
    "\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = torch.nn.functional.softmax(scores, dim=-1)\n",
    "\n",
    "        return  weights\n",
    "\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "## ==== END EVALUATION PORTION\n",
    "\n",
    "## ==== BEGIN EVALUATION PORTION\n",
    "\n",
    "class RNNEncoderDecoderLMWithAttention(torch.nn.Module):\n",
    "    \"\"\" Implements an Encoder-Decoder network, using RNN units, augmented with attention. \"\"\"\n",
    "\n",
    "    # Feel free to add additional parameters to __init__\n",
    "    def __init__(self,src_vocab_size, tgt_vocab_size, embd_dims, hidden_size, num_layers=1, dropout=0.1):\n",
    "        \"\"\" Initializes the encoder-decoder network, implemented via RNNs.\n",
    "\n",
    "        Args:\n",
    "            src_vocab_size (int): Source vocabulary size.\n",
    "            tgt_vocab_size (int): Target vocabulary size.\n",
    "            embd_dims (int): Embedding dimensions.\n",
    "            hidden_size (int): Size/Dimensions for the hidden states.\n",
    "        \"\"\"\n",
    "\n",
    "        super(RNNEncoderDecoderLMWithAttention, self).__init__()\n",
    "\n",
    "        # Dummy parameter to track the model device. Do not modify.\n",
    "        self._dummy_param = torch.nn.Parameter(torch.Tensor(0), requires_grad=False)\n",
    "\n",
    "        # BEGIN CODE : enc-dec-rnn-attn.init\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "        self.src_vocab_size  = src_vocab_size\n",
    "        self.tgt_vocab_size = tgt_vocab_size\n",
    "        self.embd_dims = embd_dims\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "\n",
    "\n",
    "        self.encoder_embed = nn.Embedding(src_vocab_size, embd_dims)\n",
    "        self.encoder_gru = nn.GRU(embd_dims, hidden_size, batch_first=True)#,bidirectional=True)\n",
    "\n",
    "        self.decoder_embedd = nn.Embedding(tgt_vocab_size,embd_dims)\n",
    "        self.attention = AttentionModule(hidden_size)\n",
    "        self.decoder_gru = nn.GRU(embd_dims+hidden_size, hidden_size,num_layers=1, batch_first=True)\n",
    "        self.decoder_fc = nn.Linear(hidden_size, tgt_vocab_size)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self._dummy_param.device\n",
    "\n",
    "    def log_probability(self, seq_x, seq_y):\n",
    "        \"\"\" Compute the conditional log probability of seq_y given seq_x, i.e., log P(seq_y | seq_x).\n",
    "\n",
    "        Args:\n",
    "            seq_x (torch.tensor): Input sequence of tokens, of shape [src_seq_len] (no batch dim)\n",
    "            seq_y (torch.tensor): Output sequence of tokens, of shape [tgt_seq_len] (no batch dim)\n",
    "\n",
    "        Returns:\n",
    "            float: Log probability of generating sequence y, given sequence x.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : enc-dec-rnn-attn.probability\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def attentions(self, seq_x, terminate_token, max_length):\n",
    "        \"\"\" Obtain attention over a sequence for decoding to the target language.\n",
    "\n",
    "        Args:\n",
    "            seq_x (torch.tensor): Tensor representing the source sequence, of shape [src_seq_len] (no batch dim)\n",
    "            terminate_token (int): Token to use as EOS, to stop generating outputs.\n",
    "            max_length (int): Maximum length to use to terminate the sampling.\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.tensor, torch.tensor]:\n",
    "                A tuple of two tensors: the attentions over individual output tokens ([tgt_seq_len, src_seq_len])\n",
    "                and the best output tokens ([tgt_seq_len]) per sequence step, based on greedy sampling.\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : rnn-enc-dec-attn.attentions\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            seq_x = seq_x.to(device)\n",
    "\n",
    "            encoder_embedded = self.dropout_layer(self.encoder_embed(seq_x.unsqueeze(0)))\n",
    "            encoder_outputs, encoder_hidden = self.encoder_gru(encoder_embedded)\n",
    "            decoder_hidden_state = encoder_hidden\n",
    "\n",
    "            attentions = []\n",
    "            outputs = []\n",
    "\n",
    "            decoder_input = (torch.tensor([0]).unsqueeze(0)).to(device)\n",
    "\n",
    "            for _ in range(max_length):\n",
    "\n",
    "                decoder_embedded = self.dropout_layer(self.decoder_embedd(decoder_input))\n",
    "\n",
    "                decoder_hidden_state = decoder_hidden_state.permute(1,0,2)\n",
    "\n",
    "                attn_weights = self.attention(decoder_hidden_state, encoder_outputs)\n",
    "\n",
    "                attentions.append(attn_weights.squeeze(1))\n",
    "\n",
    "                context = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "                decoder_input_combined = torch.cat([decoder_embedded, context], dim=2)\n",
    "\n",
    "                decoder_hidden_state = decoder_hidden_state.permute(1,0,2)\n",
    "\n",
    "                decoder_output, decoder_hidden_state = self.decoder_gru(decoder_input_combined, decoder_hidden_state)\n",
    "\n",
    "                output_probs = torch.softmax(self.decoder_fc(decoder_output), dim=-1)\n",
    "\n",
    "                top_token_index = output_probs.argmax(dim = -1)[:,-1]\n",
    "                decoder_input = top_token_index.unsqueeze(0)\n",
    "                outputs.append(top_token_index.item())\n",
    "\n",
    "                if top_token_index[0] == terminate_token or len(outputs) >= max_length:\n",
    "                    break\n",
    "\n",
    "            attentions = (decoder_input_combined.squeeze(1)).to('cpu')\n",
    "            outputs = torch.tensor(outputs).to('cpu')\n",
    "            return tuple([attentions, outputs])\n",
    "\n",
    "\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "    def forward(self, inputs, decoder_inputs=None, decoder_hidden_state=None, output_attention=False):\n",
    "        \"\"\" Performs a forward pass over the encoder-decoder network.\n",
    "\n",
    "            Accepts inputs for the encoder, inputs for the decoder, and hidden state for\n",
    "                the decoder to continue generation after the given input.\n",
    "\n",
    "        Args:\n",
    "            inputs (torch.Tensor): tensor of shape [batch_size?, src_seq_len]\n",
    "            decoder_inputs (torch.Tensor): Decoder inputs, as tensor of shape [batch_size?, 1]\n",
    "            decoder_hidden_state (any): tensor to represent decoder hidden state from time step T-1.\n",
    "            output_attention (bool): If true, this function should also return the\n",
    "                associated attention weights for the time step, of shape [batch_size?, 1, src_seq_len].\n",
    "\n",
    "        Returns:\n",
    "            tuple[torch.Tensor, any]: output from the decoder, and associated hidden state for the next step.\n",
    "\n",
    "            Decoder outputs should be log probabilities over the target vocabulary.\n",
    "\n",
    "        Example:\n",
    "        >>> model = RNNEncoderDecoderWithAttention(*args, **kwargs)\n",
    "        >>> output, hidden = model(..., output_attention=False)\n",
    "        >>> output, hidden, attn_weights = model(..., output_attention=True)\n",
    "        \"\"\"\n",
    "\n",
    "        # BEGIN CODE : enc-dec-rnn-attn.forward\n",
    "\n",
    "        # ADD YOUR CODE HERE\n",
    "\n",
    "        encoder_embedded = self.dropout_layer(self.encoder_embed(inputs))\n",
    "        encoder_outputs, encoder_hidden = self.encoder_gru(encoder_embedded)\n",
    "\n",
    "        if(decoder_hidden_state == None):\n",
    "          decoder_hidden_state = encoder_hidden\n",
    "\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        if(decoder_inputs == None):\n",
    "          decoder_inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_('<START>')\n",
    "\n",
    "        decoder_embedded = self.dropout_layer(self.decoder_embedd(decoder_inputs))\n",
    "\n",
    "        decoder_hidden_state = decoder_hidden_state.permute(1,0,2)\n",
    "\n",
    "        attn_weights = self.attention(decoder_hidden_state,encoder_outputs)\n",
    "\n",
    "        context = torch.bmm(attn_weights, encoder_outputs)\n",
    "\n",
    "\n",
    "        decoder_input_combined = torch.cat([decoder_embedded,context], dim=2)\n",
    "\n",
    "        decoder_hidden_state = decoder_hidden_state.permute(1,0,2)\n",
    "        decoder_output,decoder_hidden = self.decoder_gru(decoder_input_combined, decoder_hidden_state)\n",
    "\n",
    "        decoder_output = self.decoder_fc(decoder_output)\n",
    "\n",
    "        if output_attention:\n",
    "            return decoder_output, decoder_hidden, attn_weights\n",
    "        else:\n",
    "            return decoder_output, decoder_hidden_state\n",
    "\n",
    "\n",
    "\n",
    "        # END CODE\n",
    "\n",
    "## ==== END EVALUATION PORTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## == BEGIN EVALUATION PORTION\n",
    "\n",
    "# Edit the hyperparameters below to your desired values.\n",
    "\n",
    "# BEGIN CODE : rnn-enc-dec-attn.params\n",
    "\n",
    "# Add parameters related to the model here.\n",
    "rnn_enc_dec_attn_params = {\n",
    "    'src_vocab_size': SRC_VOCAB_SIZE+4,\n",
    "    'tgt_vocab_size': TGT_VOCAB_SIZE+4,\n",
    "    'embd_dims'     : 300,#110,\n",
    "    'hidden_size'   : 512,\n",
    "    'dropout'       : 0.6,#0.6,\n",
    "    'num_layers'    : 2\n",
    "}\n",
    "\n",
    "# Add parameters related to the dataset processing here.\n",
    "rnn_enc_dec_attn_data_params = dict(\n",
    "    src_padding=18,\n",
    "    tgt_padding=18,\n",
    ")\n",
    "\n",
    "# Add parameters related to training here.\n",
    "rnn_enc_dec_attn_training_params = dict(\n",
    "    num_epochs=50,\n",
    "    batch_size=512,\n",
    "    shuffle=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=50\n",
    ")\n",
    "\n",
    "# END CODE\n",
    "\n",
    "# Do not forget to set a deterministic seed.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = RNNEncoderDecoderLMWithAttention(**rnn_enc_dec_attn_params)\n",
    "\n",
    "# BEGIN CODE : rnn-enc-dec-attn.train\n",
    "\n",
    "# ADD YOUR CODE HERE\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.002)#0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# END CODE\n",
    "\n",
    "trainer = RNNEncoderDecoderTrainer(\n",
    "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec.attn\"),\n",
    "    model, criterion, optimizer\n",
    ")\n",
    "\n",
    "## == END EVALUATION PORTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torch.cuda.memory.empty_cache() -> None>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 / 50:  11%|█         | 50/450 [00:44<03:31,  1.89it/s, batch=5, loss=0.947]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch # 6, batch #5: loss: 0.94671196 | val_loss: 0.94544273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 / 50:  22%|██▏       | 100/450 [01:15<03:27,  1.69it/s, batch=1, loss=0.695]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #12, batch #1: loss: 0.69536471 | val_loss: 0.72081905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 / 50:  33%|███▎      | 150/450 [01:43<02:37,  1.90it/s, batch=6, loss=0.486]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #17, batch #6: loss: 0.48563609 | val_loss: 0.51933066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 / 50:  44%|████▍     | 200/450 [02:21<02:12,  1.89it/s, batch=2, loss=0.325]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #23, batch #2: loss: 0.32543397 | val_loss: 0.42413282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 / 50:  56%|█████▌    | 250/450 [02:49<01:56,  1.72it/s, batch=7, loss=0.226]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #28, batch #7: loss: 0.22580095 | val_loss: 0.37295734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 / 50:  67%|██████▋   | 300/450 [03:20<01:26,  1.74it/s, batch=3, loss=0.173]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #34, batch #3: loss: 0.17250001 | val_loss: 0.35021734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 / 50:  78%|███████▊  | 350/450 [03:51<01:06,  1.50it/s, batch=8, loss=0.141]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #39, batch #8: loss: 0.14135493 | val_loss: 0.34087666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 / 50:  89%|████████▉ | 400/450 [04:22<00:34,  1.46it/s, batch=4, loss=0.0935]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #45, batch #4: loss: 0.09350205 | val_loss: 0.34256983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 / 50: 100%|██████████| 450/450 [04:58<00:00,  1.73it/s, batch=9, loss=0.0807]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>] epoch #50, batch #9: loss: 0.08074198 | val_loss: 0.33445112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 / 50: 100%|██████████| 450/450 [04:58<00:00,  1.51it/s, batch=9, loss=0.0807]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                      : al0k\n",
      "Translation (Expected)    : आलोक\n",
      "Translation (Model)       : आलोक\n",
      "\n",
      "Name                      : sufia\n",
      "Translation (Expected)    : सूफिया\n",
      "Translation (Model)       : सूफिया\n",
      "\n",
      "Name                      : firoz\n",
      "Translation (Expected)    : फिरोज़\n",
      "Translation (Model)       : फिरोज़रोज़रोज़रोज़रोज़रोज़रोज़रो\n",
      "\n",
      "Name                      : gouri\n",
      "Translation (Expected)    : गौरी\n",
      "Translation (Model)       : गौरी\n",
      "\n",
      "Name                      : chhaya\n",
      "Translation (Expected)    : छाया\n",
      "Translation (Model)       : छाया\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change anything in the following cell.\n",
    "\n",
    "train_dataset      = TokenizerDataset(train_data     , src_tokenizer, tgt_tokenizer, **rnn_enc_dec_attn_data_params)\n",
    "validation_dataset = TokenizerDataset(validation_data, src_tokenizer, tgt_tokenizer, **rnn_enc_dec_attn_data_params)\n",
    "\n",
    "rnn_enc_dec_attn_train_data = dict(\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    collate_fn=train_dataset.collate\n",
    ")\n",
    "\n",
    "# Resume training from the last checkpoint, if interrupted midway, otherwise starts from scratch.\n",
    "#trainer.resume()\n",
    "\n",
    "# Train as per specified training parameters.\n",
    "trainer.train(**rnn_enc_dec_attn_train_data, **rnn_enc_dec_attn_training_params)\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "# Save the final model, with additional metadata.\n",
    "trainer.save(metadata={\n",
    "    'model'   : rnn_enc_dec_attn_params,\n",
    "    'data'    : rnn_enc_dec_attn_data_params,\n",
    "    'training': rnn_enc_dec_attn_training_params\n",
    "})\n",
    "\n",
    "\"\"\"We can validate the model using a few simple tests as below:\"\"\"\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "for _, row in train_data.sample(n=5, random_state=42).iterrows():\n",
    "    y_pred = rnn_greedy_generate(\n",
    "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
    "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
    "    )\n",
    "\n",
    "    print(\"Name                      :\", row['Name'])\n",
    "    print(\"Translation (Expected)    :\", row['Translation'])\n",
    "    print(\"Translation (Model)       :\", y_pred)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                      : mhosin\n",
      "Translation (Expected)    : मोहसीन\n",
      "Translation (Model)       : एमएमएमएमएमएमएमएमए\n",
      "\n",
      "Name                      : qadeem\n",
      "Translation (Expected)    : क़दीम\n",
      "Translation (Model)       : क़ाम\n",
      "\n",
      "Name                      : ashiqu\n",
      "Translation (Expected)    : आशिक़\n",
      "Translation (Model)       : आशिकूशीकूशीकूशीकूशीकूशी\n",
      "\n",
      "Name                      : midhana\n",
      "Translation (Expected)    : मिधना\n",
      "Translation (Model)       : मीना\n",
      "\n",
      "Name                      : divakar\n",
      "Translation (Expected)    : दिवाकर\n",
      "Translation (Model)       : दिवकर\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Please do not change anything in the following cell.\n",
    "\n",
    "for _, row in validation_data.sample(n=5, random_state=42).iterrows():\n",
    "    y_pred = rnn_greedy_generate(\n",
    "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
    "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
    "    )\n",
    "\n",
    "    print(\"Name                      :\", row['Name'])\n",
    "    print(\"Translation (Expected)    :\", row['Translation'])\n",
    "    print(\"Translation (Model)       :\", y_pred)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 108 (l) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 112 (p) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 60 (<) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 83 (S) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 84 (T) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 65 (A) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 82 (R) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 62 (>) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 107 (k) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 117 (u) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 115 (s) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 109 (m) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 97 (a) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 91 ([) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 69 (E) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 79 (O) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 93 (]) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 100 (d) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 101 (e) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 114 (r) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 110 (n) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 104 (h) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/events.py:82: UserWarning: Glyph 102 (f) missing from font(s) Samyak Devanagari.\n",
      "  func(*args, **kwargs)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 108 (l) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 112 (p) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 60 (<) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 83 (S) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 84 (T) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 65 (A) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 82 (R) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 62 (>) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 107 (k) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 117 (u) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 115 (s) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 109 (m) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 97 (a) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 91 ([) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 69 (E) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 79 (O) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 93 (]) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 100 (d) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 101 (e) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 114 (r) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 110 (n) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 104 (h) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/data2/home/kpnaveen/anaconda3/envs/DLNLP_A1/lib/python3.11/site-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 102 (f) missing from font(s) Samyak Devanagari.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"It may also be useful to look at attention maps for different examples:\"\"\"\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "def visualize_attention(src_glyphs, tgt_glyphs, attention, axes):\n",
    "    axes.matshow(attention.numpy(), cmap='bone')\n",
    "\n",
    "    axes.set_xticks(numpy.arange(len(src_glyphs)), labels=src_glyphs)\n",
    "    axes.set_yticks(numpy.arange(len(tgt_glyphs)), labels=tgt_glyphs)\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "pyplot.figure(figsize=(12, 10))\n",
    "\n",
    "src_id_to_token = inverse_vocabulary(src_tokenizer)\n",
    "tgt_id_to_token = inverse_vocabulary(tgt_tokenizer)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, row in train_data.sample(n=4, random_state=69, ignore_index=True).iterrows():\n",
    "        src_tokens = torch.tensor(src_tokenizer.encode(row['Name']))\n",
    "        attentions, tgt_tokens = model.attentions(src_tokens, tgt_tokenizer.get_special_tokens()['[EOS]'], max_length=50)\n",
    "        src_glyphs = apply_inverse_vocab(src_tokens.tolist(), src_id_to_token)\n",
    "        tgt_glyphs = apply_inverse_vocab(tgt_tokens.tolist(), tgt_id_to_token)\n",
    "        axes = pyplot.subplot(2, 2, i+1)\n",
    "        visualize_attention(src_glyphs, tgt_glyphs, attentions, axes)\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "output_data = []\n",
    "for _, row in validation_data.iterrows():\n",
    "    y_pred = rnn_greedy_generate(\n",
    "        model, row['Name'], src_tokenizer, tgt_tokenizer,\n",
    "        max_length = rnn_enc_dec_data_params['tgt_padding']\n",
    "    )\n",
    "    output_data.append({ 'Name': row['Name'], 'Translation': y_pred })\n",
    "\n",
    "pd.DataFrame.from_records(output_data).to_csv(\n",
    "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec.attn\", \"outputs.csv\"), index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change anything in the following cell.\n",
    "\n",
    "# Release resources\n",
    "if 'trainer' in globals():\n",
    "    del trainer\n",
    "\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "\n",
    "sync_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Evaluation\n",
    "\n",
    "In the last few modules, you have implemented different approaches towards transliteration of Indian names to Hindi. To assess how well different systems perform, it is useful to compute different metrics, which assess different properties:\n",
    "\n",
    "- **Accuracy**: From a parallel corpus, number of translations the model got exactly right. Higher the better. Note that this makes sense only for this task. and lacks granularity.\n",
    "- **Edit Distance**: Number of edits at the character level (insertions, deletions, substitutions) required to transform your model's outputs to a reference translation. Lower the better.\n",
    "- **Character Error Rate (CER)**: The rate at which your system/model makes mistakes at the character level. Lower the better.\n",
    "- **Token Error Rate (TER)**: The rate at which your system/model makes mistakes at the token level. Lower the better. Depending on your tokenizer implementation, could be the same as CER.\n",
    "- **BiLingual Evaluation Understudy (BLEU)**: Proposed by [Papineni et al., 2002](https://aclanthology.org/P02-1040/), BLEU is a metric that assess the quality of a translation against reference translations through assessing n-gram overlap. Higher the better.\n",
    "\n",
    "Since accents and half-letters exist as separate characters in the Unicode specification, and can change the interpretation of the output, metrics that operate at the character level will treat these separately.\n",
    "\"\"\"\n",
    "\n",
    "# Please do not change anything in the following cell.\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\" Class to handle all the logic concerning the evaluation of trained models.  \"\"\"\n",
    "\n",
    "    def __init__(self, src_tokenizer, tgt_tokenizer) -> None:\n",
    "        \"\"\" Initializes the evaluator.\n",
    "\n",
    "        Args:\n",
    "            src_tokenizer (Tokenizer): Tokenizer for input strings in the source language.\n",
    "            tgt_tokenizer (Tokenizer): Tokenizer for output strings in the target language.\n",
    "        \"\"\"\n",
    "\n",
    "        self.device = 'cuda:3' if torch.cuda.is_available() else 'cpu'\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "        self.decoding_method = None\n",
    "\n",
    "    def set_decoding_method(self, decoding_method):\n",
    "        \"\"\" Sets the decoding method to use with models.\n",
    "                The evaluation function will use the set decoding method to generate outputs from the model.\n",
    "\n",
    "        Args:\n",
    "            decoding_method (function): Decoding method.\n",
    "                Must accept the model instance, the input string, and tokenizers as arguments.\n",
    "                Can accept additional arguments if required.\n",
    "        \"\"\"\n",
    "\n",
    "        self.decoding_method = decoding_method\n",
    "\n",
    "    @staticmethod\n",
    "    def decompose(string):\n",
    "        \"\"\" Decomposes a string into a set of tokens.\n",
    "\n",
    "        Args:\n",
    "            string (str): String to decompose.\n",
    "\n",
    "        Returns:\n",
    "            list[str]: List of characters from the string.\n",
    "        \"\"\"\n",
    "        return unicodedata.normalize('NFKD', string).encode('utf-8')\n",
    "\n",
    "    @staticmethod\n",
    "    def levenshtein_distance(string1, string2):\n",
    "        \"\"\" Computes the levensthein distance between two strings.\n",
    "\n",
    "        Args:\n",
    "            string1 (list[any]): Sequence A.\n",
    "            string2 (list[any]): Sequence B.\n",
    "\n",
    "        Returns:\n",
    "            tuple[int, int, int]: Number of insertions + deletions, substitutions and no-ops.\n",
    "        \"\"\"\n",
    "\n",
    "        costs = [\n",
    "            [ 0 for j in range(len(string2)+1) ]\n",
    "            for i in range(len(string1)+1)\n",
    "        ]\n",
    "\n",
    "        # Prepare matrix of costs.\n",
    "        for i in range(len(string1)+1): costs[i][0] = i\n",
    "        for j in range(len(string2)+1): costs[0][j] = j\n",
    "        for i in range(1, len(string1)+1):\n",
    "            for j in range(1, len(string2)+1):\n",
    "                costs[i][j] = min(\n",
    "                    costs[i][j-1] + 1,\n",
    "                    costs[i-1][j] + 1,\n",
    "                    costs[i-1][j-1] + (0 if string1[i-1] == string2[j-1] else 1)\n",
    "                )\n",
    "\n",
    "        # Decode matrix in backward manner for actual operation counts.\n",
    "        c_ins_del, c_sub, c_noop = 0, 0, 0\n",
    "\n",
    "        i, j = len(string1), len(string2)\n",
    "        while i > 0 or j > 0:\n",
    "            if i > 0 and costs[i][j] == costs[i-1][j] + 1:\n",
    "                c_ins_del += 1\n",
    "                i -= 1\n",
    "            elif j > 0 and costs[i][j] == costs[i][j-1] + 1:\n",
    "                c_ins_del += 1\n",
    "                j -= 1\n",
    "            elif i > 0 and j > 0:\n",
    "                if string1[i-1] == string2[j-1]:\n",
    "                    c_noop += 1\n",
    "                else:\n",
    "                    c_sub += 1\n",
    "                i, j = i-1, j-1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        return c_ins_del, c_sub, c_noop\n",
    "\n",
    "    @staticmethod\n",
    "    def accuracy(y_true, y_pred):\n",
    "        \"\"\" Computes the accuracy of the predictions, against a reference set of predictions.\n",
    "\n",
    "        Args:\n",
    "            y_true (list[str]): Actual translations.\n",
    "            y_pred (list[str]): Generated translations.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy score, between 0 and 1.\n",
    "        \"\"\"\n",
    "        return sum(yi_true == yi_pred for yi_true, yi_pred in zip(y_true, y_pred)) / len(y_pred)\n",
    "\n",
    "    @classmethod\n",
    "    def char_error_rate(cls, y_true, y_pred):\n",
    "        \"\"\" Computes the character level error rate (CER) of the set of\n",
    "            predictions against the reference translations.\n",
    "\n",
    "        Args:\n",
    "            y_true (list[str]): Actual translations.\n",
    "            y_pred (list[str]): Generated translations.\n",
    "\n",
    "        Returns:\n",
    "            float: CER score, between 0 and 1. Lower the better.\n",
    "        \"\"\"\n",
    "\n",
    "        cer_score = 0\n",
    "\n",
    "        for yi_true, yi_pred in zip(y_true, y_pred):\n",
    "            yi_true, yi_pred = cls.decompose(yi_true), cls.decompose(yi_pred)\n",
    "            c_ins_del, c_sub, c_noop = cls.levenshtein_distance(yi_true, yi_pred)\n",
    "            cer_score += (c_ins_del + c_sub) / (c_ins_del + c_sub + c_noop)\n",
    "\n",
    "        return cer_score / len(y_true)\n",
    "\n",
    "    def token_error_rate(self, y_true, y_pred):\n",
    "        \"\"\" Computes the token level error rate (TER) of the set of\n",
    "            predictions against the reference translations.\n",
    "\n",
    "        Args:\n",
    "            y_true (list[str]): Actual translations.\n",
    "            y_pred (list[str]): Generated translations.\n",
    "\n",
    "        Returns:\n",
    "            float: TER score, between 0 and 1. Lower the better.\n",
    "        \"\"\"\n",
    "\n",
    "        ter_score = 0\n",
    "\n",
    "        for yi_true, yi_pred in zip(y_true, y_pred):\n",
    "            yi_true = self.tgt_tokenizer.encode(yi_true, add_start=False, add_end=False)\n",
    "            yi_pred = self.tgt_tokenizer.encode(yi_pred, add_start=False, add_end=False)\n",
    "            t_ins_del, t_sub, t_noop = self.levenshtein_distance(yi_true, yi_pred)\n",
    "            ter_score += (t_ins_del + t_sub) / (t_ins_del + t_sub + t_noop)\n",
    "\n",
    "        return ter_score / len(y_true)\n",
    "\n",
    "    @classmethod\n",
    "    def bleu_score(cls, y_true, y_pred):\n",
    "        \"\"\" Computes the average BLEU score of the set of predictions against the reference translations.\n",
    "\n",
    "            Uses default parameters and equal weights for all n-grams, with max N = 4. (Thus computes BLEU-4).\n",
    "            Uses a smoothing method for the case of missing n-grams.\n",
    "\n",
    "        Args:\n",
    "            y_true (list[str]): Actual translations.\n",
    "            y_pred (list[str]): Generated translations.\n",
    "\n",
    "        Returns:\n",
    "            float: BLEU-4 score, the higher the better.\n",
    "        \"\"\"\n",
    "\n",
    "        y_true = [ [ cls.decompose(yi) ] for yi in y_true ]\n",
    "        y_pred = [ cls.decompose(yi) for yi in y_pred ]\n",
    "\n",
    "        smoothing = bleu_score.SmoothingFunction()\n",
    "\n",
    "        return bleu_score.corpus_bleu(\n",
    "            y_true, y_pred,\n",
    "            smoothing_function=smoothing.method1\n",
    "        )\n",
    "\n",
    "    def evaluate(self, model_path, data, reference_outputs, **decoding_kwargs):\n",
    "        \"\"\" Performs the evaluation of a specified model over given data.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to load the model from. Must have a model.pt file.\n",
    "            data (list[str]): List of input strings to translate.\n",
    "            reference_outputs (list[str]): List of output strings to use as reference.\n",
    "            decoding_kwargs (dict[str, any]): Additional arguments to forward to the decoding method.\n",
    "                This could be for instance, max_length for a greedy decoding method.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the decoding method is not set apriori.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.decoding_method is None:\n",
    "            raise ValueError(f\"{self.evaluate.__name__}: no decoding method is set, assign before use.\")\n",
    "\n",
    "        # Load the model to the active device.\n",
    "        model = torch.load(os.path.join(model_path, 'model.pt'), map_location=self.device)\n",
    "\n",
    "        # Set model use parameters.\n",
    "        model.to(self.device)\n",
    "        model.eval()\n",
    "\n",
    "        # Generate outputs.\n",
    "        generated_outputs = []\n",
    "        with torch.no_grad():\n",
    "            for seq_x in data:\n",
    "                generated_outputs.append(self.decoding_method(\n",
    "                    model, seq_x, self.src_tokenizer,\n",
    "                    self.tgt_tokenizer, **decoding_kwargs\n",
    "                ))\n",
    "\n",
    "        accuracy_score = self.accuracy(reference_outputs, generated_outputs)\n",
    "        cer_score      = self.char_error_rate(reference_outputs, generated_outputs)\n",
    "        ter_score      = self.token_error_rate(reference_outputs, generated_outputs)\n",
    "        blue_score     = self.bleu_score(reference_outputs, generated_outputs)\n",
    "\n",
    "        print(\"EVALUATION:\", \">\", \"accuracy:\", f\"{accuracy_score:.2%}\")\n",
    "        print(\"EVALUATION:\", \">\", \"CER     :\", f\"{cer_score:.2%}\")\n",
    "        print(\"EVALUATION:\", \">\", \"TER     :\", f\"{ter_score:.2%}\")\n",
    "        print(\"EVALUATION:\", \">\", \"BLEU    :\", f\"{blue_score:.4f}\")\n",
    "        print()\n",
    "\n",
    "        # Free resources once evaluation is complete.\n",
    "        del model\n",
    "        sync_vram()\n",
    "\n",
    "# Please do not change anything in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(src_tokenizer, tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: enc-dec-rnn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3838105/3311784119.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(os.path.join(model_path, 'model.pt'), map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: > accuracy: 13.25%\n",
      "EVALUATION: > CER     : 33.77%\n",
      "EVALUATION: > TER     : 51.12%\n",
      "EVALUATION: > BLEU    : 0.5757\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use greedy decoding for producing outputs.\n",
    "evaluator.set_decoding_method(rnn_greedy_generate)\n",
    "\n",
    "# Evaluate enc-dec-rnn\n",
    "print(\"EVALUATION:\", \"enc-dec-rnn\")\n",
    "evaluator.evaluate(\n",
    "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec\"),\n",
    "    validation_data['Name'], validation_data['Translation'],\n",
    "    max_length = rnn_enc_dec_data_params['tgt_padding']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: enc-dec-rnn-attn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3838105/3311784119.py:206: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(os.path.join(model_path, 'model.pt'), map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION: > accuracy: 22.00%\n",
      "EVALUATION: > CER     : 33.97%\n",
      "EVALUATION: > TER     : 49.00%\n",
      "EVALUATION: > BLEU    : 0.4132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate enc-dec-rnn-attn\n",
    "print(\"EVALUATION:\", \"enc-dec-rnn-attn\")\n",
    "evaluator.evaluate(\n",
    "    os.path.join(DIRECTORY_NAME, \"rnn.enc-dec.attn\"),\n",
    "    validation_data['Name'], validation_data['Translation'],\n",
    "    max_length = rnn_enc_dec_attn_data_params['tgt_padding']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
